{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593da0e7-bdce-430c-97b1-8a378f94f14d",
   "metadata": {},
   "source": [
    "### **1. The Optimization Problem**\n",
    "We aim to minimize the **sum of squared residuals**:\n",
    "\n",
    "$\n",
    "\\min_x \\|r(x)\\|^2 = \\min_x \\sum_{i=1}^N r_i(x)^2\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $x $: The parameter we are optimizing.\n",
    "- $r_i(x) $: The residual for the $i $-th observation, which is a function of $x $.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Taylor Expansion**\n",
    "The residual function $r_i(x) $ is generally nonlinear. To simplify optimization, we approximate $r_i(x) $ around the current estimate $x_k $ using a first-order Taylor expansion:\n",
    "\n",
    "$\n",
    "r_i(x) \\approx r_i(x_k) + J_i(x_k)(x - x_k)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $r_i(x_k) $: The residual at the current estimate $x_k $.\n",
    "- $J_i(x_k) = \\frac{\\partial r_i}{\\partial x} $: The Jacobian of the residual with respect to $x $.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Objective Function Approximation**\n",
    "The cost function can then be approximated (locally) as:\n",
    "\n",
    "$\n",
    "\\|r(x)\\|^2 \\approx \\|r(x_k) + J(x_k)(x - x_k)\\|^2\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $r(x) $ is the vector of all residuals.\n",
    "- $J(x) $ is the Jacobian matrix, containing the derivatives of all residuals with respect to $x $.\n",
    "\n",
    "Expanding this:\n",
    "$\n",
    "\\|r(x_k) + J(x_k)(x - x_k)\\|^2 = \\|r(x_k)\\|^2 + 2(x - x_k)^T J(x_k)^T r(x_k) + (x - x_k)^T J(x_k)^T J(x_k)(x - x_k)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Minimizing the Quadratic Approximation**\n",
    "To minimize this quadratic approximation, take the derivative with respect to $x $ and set it to zero:\n",
    "\n",
    "$\n",
    "\\nabla \\|r(x)\\|^2 = 2 J(x_k)^T r(x_k) + 2 J(x_k)^T J(x_k)(x - x_k) = 0\n",
    "$\n",
    "\n",
    "Simplifying:\n",
    "$\n",
    "J(x_k)^T J(x_k)(x - x_k) = -J(x_k)^T r(x_k)\n",
    "$\n",
    "\n",
    "Solve for the new $x $:\n",
    "$\n",
    "x_{k+1} = x_k - (J(x_k)^T J(x_k))^{-1} J(x_k)^T r(x_k)\n",
    "$\n",
    "\n",
    "This is the **Gauss-Newton update rule**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Key Terms in Gauss-Newton Update**\n",
    "\n",
    "- $J(x_k) $: The Jacobian of residuals with respect to parameters.\n",
    "- $J(x_k)^T J(x_k) $: The approximate Hessian matrix of the cost function.\n",
    "- $J(x_k)^T r(x_k) $: The gradient of the cost function.\n",
    "- $(J(x_k)^T J(x_k))^{-1} J(x_k)^T r(x_k) $: The step direction to minimize the cost.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Limitations of Gauss-Newton**\n",
    "\n",
    "- **Convergence Issues**: Gauss-Newton works well if the residuals are small and the cost function is close to quadratic. If the problem is highly nonlinear or has large residuals, it may fail to converge.\n",
    "- **Hessian Approximation**: Gauss-Newton assumes the Hessian can be approximated as $J^T J $, which ignores second-order terms. For highly nonlinear problems, this can lead to inaccuracies.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Damped Gauss-Newton: Levenberg-Marquardt**\n",
    "\n",
    "To address these issues, Ceres Solver often uses the **Levenberg-Marquardt algorithm**, which combines Gauss-Newton with a damping term:\n",
    "\n",
    "$\n",
    "(J^T J + \\lambda I)(x_{k+1} - x_k) = -J^T r(x_k)\n",
    "$\n",
    "\n",
    "Where $\\lambda $ controls the damping:\n",
    "- If $\\lambda $ is small, it behaves like Gauss-Newton.\n",
    "- If $\\lambda $ is large, it behaves more like gradient descent, ensuring stability.\n",
    "\n",
    "---\n",
    "\n",
    "### **In Summary**\n",
    "- Ceres Solver typically enhances this with the **Levenberg-Marquardt method** for better convergence on highly nonlinear problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbdce12-d9b2-42a8-afb3-1109a59fc0c1",
   "metadata": {},
   "source": [
    "### **1. What is a Residual Block?**\n",
    "A **residual block** represents an individual error term in the optimization problem. The goal of optimization is to minimize the sum of squared residuals, which is commonly written as:\n",
    "\n",
    "$\n",
    "\\min_{\\mathbf{x}} \\sum_{i=1}^{N} \\rho_i\\left( \\|r_i(\\mathbf{x})\\|^2 \\right)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x} $ is the vector of parameters being optimized.\n",
    "- $r_i(\\mathbf{x}) $ is the **residual** of the $i $-th residual block.\n",
    "- $\\rho_i $ is an optional **robust loss function** (e.g., Huber loss), which reduces the influence of outliers. If no loss function is used, $\\rho_i(z) = z $.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Parameter Block**\n",
    "A **parameter block** refers to the set of variables that a residual block depends on. For instance, if a residual block depends on a single scalar variable $x $, then $x $ is the parameter block for that residual block.\n",
    "\n",
    "In Ceres:\n",
    "- A residual block computes $r_i(\\mathbf{x}) $ (and its Jacobian with respect to $\\mathbf{x} $).\n",
    "- A parameter block holds the variables $\\mathbf{x} $ that the residual block modifies during optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Equation Example**\n",
    "\n",
    "Let's consider a simple problem:\n",
    "\n",
    "$\n",
    "r(x) = x^2 - 4\n",
    "$\n",
    "\n",
    "Here:\n",
    "- **Residual block**: Computes the residual $r(x) = x^2 - 4 $.\n",
    "- **Parameter block**: $x $, which is the scalar value being optimized.\n",
    "\n",
    "The optimization problem is:\n",
    "$\n",
    "\\min_x \\| r(x) \\|^2 = \\min_x (x^2 - 4)^2\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **In Code**\n",
    "\n",
    "1. **Define the Cost Functor** (Residual Block):\n",
    "```cpp\n",
    "struct CostFunctor {\n",
    "  template <typename T>\n",
    "  bool operator()(const T* const x, T* residual) const {\n",
    "    residual[0] = x[0] * x[0] - T(4.0); // r(x) = x^2 - 4\n",
    "    return true;\n",
    "  }\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d191c-f4ff-4b0c-8636-7fdf9fb38228",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The declaration `const T* const x` is interpreted as follows:\n",
    "\n",
    "1. **`const T*`**:\n",
    "   - The pointer `x` points to an object of type `T` that is constant.\n",
    "   - You cannot modify the value of the object that `x` points to via `x`.\n",
    "\n",
    "2. **`const x`**:\n",
    "   - The pointer `x` itself is constant, meaning you cannot change the value of the pointer `x` to point to another address.\n",
    "\n",
    "Together, `const T* const x` means:\n",
    "- The object being pointed to is constant, so its value cannot be changed.\n",
    "- The pointer itself is constant, so it cannot be reassigned to point to a different address.\n",
    "\n",
    "### Why is this used?\n",
    "This declaration enforces immutability of both:\n",
    "1. The pointer (`x`) itself.\n",
    "2. The value(s) being pointed to by the pointer.\n",
    "\n",
    "In the context of the `operator()` function in the `CostFunctor` struct:\n",
    "- `const T* const x` ensures that the input parameter `x` cannot be accidentally modified within the function, either by changing what `x` points to or by modifying the contents of `x`.\n",
    "\n",
    "### Summary\n",
    "- The first `const` applies to the object being pointed to (`T`).\n",
    "- The second `const` applies to the pointer itself.\n",
    "- It enforces a strong guarantee that `x` and the data it points to remain immutable within the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e90cbf-a7ff-4ded-90fa-29f9c034d5cc",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- `const T* const x`: Pointer to the parameter block (the variable $x $).\n",
    "- `T* residual`: Pointer to the residual (output of $r(x) $).\n",
    "\n",
    "This is equivalent to $r(x) = x^2 - 4 $.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Set Up the Parameter Block and Problem**:\n",
    "```cpp\n",
    "double initial_x = 5.0;  // Initial value of the parameter\n",
    "double x = initial_x;    // Parameter block (modifiable during optimization)\n",
    "\n",
    "Problem problem;  // Create a Ceres problem\n",
    "```\n",
    "\n",
    "Here:\n",
    "- `x` is the **parameter block**, and its value will be modified by the solver.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Add the Residual Block**:\n",
    "```cpp\n",
    "CostFunction* cost_function =\n",
    "    new AutoDiffCostFunction<CostFunctor, 1, 1>();  // r(x) with 1 residual and 1 parameter\n",
    "problem.AddResidualBlock(cost_function, nullptr, &x);  // Add to the problem\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- `AutoDiffCostFunction<CostFunctor, 1, 1>()`: Defines the residual block, specifying:\n",
    "- `1`: The number of residuals (output of the cost function). Here, there is **1 residual**.\n",
    "- `1`: The number of parameters (input to the cost function). Here, the optimization variable (`x`) has **1 parameter**.\n",
    "- `nullptr`: This is for the loss function, No loss function ($\\rho_i(z) = z $). If you want robust error handling (e.g., to down-weight outliers), you would pass a `LossFunction` here. Passing `nullptr` means no loss function is used (default least squares).\n",
    "- `&x`: The pointer to the parameter block being optimized. Ceres modifies the value of `x` during optimization to minimize the residual.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Solver Minimization**\n",
    "\n",
    "The Ceres solver minimizes the sum of squared residuals:\n",
    "\n",
    "$\n",
    "\\min_x \\|r(x)\\|^2 = \\min_x (x^2 - 4)^2\n",
    "$\n",
    "\n",
    "This is achieved by:\n",
    "```cpp\n",
    "Solver::Options options;\n",
    "options.minimizer_progress_to_stdout = true;\n",
    "\n",
    "Solver::Summary summary;\n",
    "Solve(options, &problem, &summary);\n",
    "\n",
    "std::cout << \"Initial x: \" << initial_x << \", Optimized x: \" << x << \"\\n\";\n",
    "```\n",
    "\n",
    "The solver modifies `x` iteratively to minimize $(x^2 - 4)^2 $, finding the optimal $x $ that minimizes the error.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary: Residual Block vs Parameter Block**\n",
    "\n",
    "| Concept            | Mathematical Meaning                                | Code Representation           |\n",
    "|--------------------|----------------------------------------------------|--------------------------------|\n",
    "| **Residual Block** | $r(x) = x^2 - 4 $                               | `AutoDiffCostFunction`        |\n",
    "| **Parameter Block**| The variable $x $ being optimized               | `double x` passed by pointer  |\n",
    "| **Loss Function**  | Optional $\\rho_i $, reduces outlier influence   | `nullptr` (no loss function)  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf9eb2-36a8-4c17-a2be-1c26c7841323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
