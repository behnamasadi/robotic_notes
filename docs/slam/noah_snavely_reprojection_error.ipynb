{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2943779c-4332-41e7-b238-b94d4f789d63",
   "metadata": {},
   "source": [
    "## Noah Snavely reprojection error\n",
    "\n",
    "\n",
    "```cpp\n",
    "SnavelyReprojectionError struct SnavelyReprojectionError {\n",
    "  SnavelyReprojectionError(double observed_x, double observed_y)\n",
    "      : observed_x(observed_x), observed_y(observed_y) {}\n",
    "  template <typename T>\n",
    "  bool operator()(const T* const camera,\n",
    "                  const T* const point,\n",
    "                  T* residuals) const {\n",
    "    // camera[0,1,2] are the angle-axis rotation.\n",
    "    T p[3];\n",
    "    AngleAxisRotatePoint(camera, point, p);\n",
    "    // camera[3,4,5] are the translation.\n",
    "    p[0] += camera[3];\n",
    "    p[1] += camera[4];\n",
    "    p[2] += camera[5];\n",
    "    // Compute the center of distortion. The sign change comes from\n",
    "    // the camera model that Noah Snavely's Bundler assumes, whereby\n",
    "    // the camera coordinate system has a negative z axis.\n",
    "    const T xp = - p[0] / p[2];\n",
    "    const T yp = - p[1] / p[2];\n",
    "    // Apply second and fourth order radial distortion.\n",
    "    const T& l1 = camera[7];\n",
    "    const T& l2 = camera[8];\n",
    "    const T r2 = xp*xp + yp*yp;\n",
    "    const T distortion = T(1.0) + r2  * (l1 + l2  * r2);\n",
    "    // Compute final projected point position.\n",
    "    const T& focal = camera[6];\n",
    "    const T predicted_x = focal * distortion * xp;\n",
    "    const T predicted_y = focal * distortion * yp;\n",
    "    // The error is the difference between the predicted and observed position.\n",
    "    residuals[0] = predicted_x - T(observed_x);\n",
    "    residuals[1] = predicted_y - T(observed_y);\n",
    "    return true;\n",
    "  }\n",
    "  // Factory to hide the construction of the CostFunction object from\n",
    "  // the client code.\n",
    "  static ceres::CostFunction* Create(const double observed_x,\n",
    "                                     const double observed_y) {\n",
    "    return (new ceres::AutoDiffCostFunction<SnavelyReprojectionError, 2, 9, 3>(\n",
    "                new SnavelyReprojectionError(observed_x, observed_y)));\n",
    "  }\n",
    "  double observed_x;\n",
    "  double observed_y;\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ceab7e-c8a2-493b-bf1e-ca7a2e306c9d",
   "metadata": {},
   "source": [
    "The `SnavelyReprojectionError` particularly used in Ceres Solver. This struct models the reprojection error in terms of the difference between observed and projected pixel coordinates.\n",
    "\n",
    "\n",
    "\n",
    "1. **Input Observations (`observed_x`, `observed_y`)**:\n",
    "   - These are the measured pixel coordinates in the image, provided as inputs during initialization of the struct.\n",
    "\n",
    "2. **Camera Parameters (`camera`)**:\n",
    "   - The `camera` parameter is a 9-dimensional array containing:\n",
    "     - **[0-2]**: Rotation in angle-axis representation.\n",
    "     - **[3-5]**: Translation vector components.\n",
    "     - **[6]**: Focal length.\n",
    "     - **[7-8]**: Radial distortion coefficients (second-order `l1` and fourth-order `l2`).\n",
    "\n",
    "3. **Point Parameters (`point`)**:\n",
    "   - The `point` parameter is a 3-dimensional array representing the 3D world point in space.\n",
    "\n",
    "4. **Steps to Project 3D Points to Pixel Coordinates**:\n",
    "   - **Rotation**: The 3D point is rotated using the angle-axis representation provided in `camera[0-2]`.\n",
    "   - **Translation**: The rotated point is translated using the values in `camera[3-5]`.\n",
    "   - **Perspective Division**: The camera assumes a pinhole model, so the 3D point is converted to normalized image coordinates `(xp, yp)`:\n",
    "     $\n",
    "     xp = -p[0] / p[2], \\quad yp = -p[1] / p[2]\n",
    "     $\n",
    "   - **Radial Distortion**: The normalized coordinates are adjusted using a radial distortion model:\n",
    "     $\n",
    "     r^2 = xp^2 + yp^2\n",
    "     $\n",
    "     $\n",
    "     distortion = 1 + r^2 \\cdot (l1 + l2 \\cdot r^2)\n",
    "     $\n",
    "     The distorted coordinates are:\n",
    "     $\n",
    "     xp' = distortion \\cdot xp, \\quad yp' = distortion \\cdot yp\n",
    "     $\n",
    "   - **Scaling to Pixels**: The distorted coordinates are scaled by the focal length to get the projected pixel coordinates:\n",
    "     $\n",
    "     predicted_x = focal \\cdot xp', \\quad predicted_y = focal \\cdot yp'\n",
    "     $\n",
    "\n",
    "5. **Residuals (Error)**:\n",
    "   - The reprojection error is calculated as the difference between the projected pixel coordinates and the observed pixel coordinates:\n",
    "     $\n",
    "     residuals[0] = predicted_x - observed_x\n",
    "     $\n",
    "     $\n",
    "     residuals[1] = predicted_y - observed_y\n",
    "     $\n",
    "\n",
    "\n",
    "### Principal Point in the Bundler Model\n",
    "The `SnavelyReprojectionError` implementation does not explicitly include the principal point offsets $ c_x $ and $ c_y $ in the computation of pixel coordinates. \n",
    "\n",
    "According to the [Bundler manual](https://www.cs.cornell.edu/~snavely/bundler/bundler-v0.4-manual.html), the camera model assumes:\n",
    "$\n",
    "\\text{Pixel coordinates} = \\begin{bmatrix} f & 0 & c_x \\\\ 0 & f & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\cdot \\text{Normalized coordinates}\n",
    "$\n",
    "Where:\n",
    "- $ f $: Focal length.\n",
    "- $ c_x, c_y $: Principal point offsets in the image plane.\n",
    "\n",
    "### Why $ c_x $ and $ c_y $ Are Missing in This Implementation\n",
    "In your provided implementation:\n",
    "1. The predicted pixel coordinates are computed as:\n",
    "   $\n",
    "   predicted_x = f \\cdot \\text{distorted } x\n",
    "   $\n",
    "   $\n",
    "   predicted_y = f \\cdot \\text{distorted } y\n",
    "   $\n",
    "   There is no explicit addition of $ c_x $ and $ c_y $, which would look like:\n",
    "   $\n",
    "   predicted_x = f \\cdot \\text{distorted } x + c_x\n",
    "   $\n",
    "   $\n",
    "   predicted_y = f \\cdot \\text{distorted } y + c_y\n",
    "   $\n",
    "\n",
    "2. This suggests that the implementation assumes the optical center ($ c_x, c_y $) is at the image center, i.e., $ c_x = c_y = 0 $. This is a simplification and might not be valid for all camera setups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508855ce-e85f-45f9-b66a-3087d7b87952",
   "metadata": {},
   "source": [
    "The formula for projecting a 3D point $\\mathbf{X} = [X, Y, Z]^T$ into a camera with parameters $R_w^c$, $t_w^c$ ( pose of the **world frame expressed in the camera frame), $f$ (focal length), and radial distortion coefficients $k_1$ and $k_2$ is as follows:\n",
    "\n",
    "1. **Transform the 3D point into the camera coordinate system**:\n",
    "   $\n",
    "   \\mathbf{X}_c = R_w^c \\mathbf{X} + t_w^c\n",
    "   $\n",
    "   Where:\n",
    "   - $\\mathbf{X}_c = [X_c, Y_c, Z_c]^T$ is the point in the camera coordinate system.\n",
    "   - $R_w^c$ is the 3x3 rotation matrix.\n",
    "   - $t_w^c = [t_x, t_y, t_z]^T$ is the translation vector.\n",
    "\n",
    "2. **Project the point into normalized image coordinates**:\n",
    "   $\n",
    "   x = \\frac{X_c}{Z_c}, \\quad y = \\frac{Y_c}{Z_c}\n",
    "   $\n",
    "\n",
    "3. **Apply radial distortion**:\n",
    "   Compute the radial distance $r^2$:\n",
    "   $\n",
    "   r^2 = x^2 + y^2\n",
    "   $\n",
    "   Apply the radial distortion model:\n",
    "   $\n",
    "   x' = x \\cdot \\left(1 + k_1 r^2 + k_2 r^4\\right)\n",
    "   $\n",
    "   $\n",
    "   y' = y \\cdot \\left(1 + k_1 r^2 + k_2 r^4\\right)\n",
    "   $\n",
    "   Where:\n",
    "   - $k_1$ and $k_2$ are the first and second radial distortion coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd127ca-57bf-4486-8793-e5649314658e",
   "metadata": {},
   "source": [
    "In a standard incremental SfM pipeline that uses something like the Snavely/BAL (Bundle Adjustment in the Large) reprojection model, *all cameras* and *all 3D points* should typically be expressed in a common “world” coordinate system (often chosen to be the first camera).  Whenever you add a new camera (e.g., camera 3) and triangulate new points in that camera’s local frame, you should transform both the camera’s pose and the newly triangulated points into the common reference frame before adding them to the bundle adjuster.\n",
    "\n",
    "Below is a conceptual explanation of **why** and **how** to do that.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Snavely Reprojection Model Assumes a Common World Frame\n",
    "\n",
    "In the snippet of `SnavelyReprojectionError` you showed, we see:\n",
    "```cpp\n",
    "AngleAxisRotatePoint(camera, point, p);  \n",
    "p[0] += camera[3];\n",
    "p[1] += camera[4];\n",
    "p[2] += camera[5];\n",
    "```\n",
    "Here,\n",
    "- `camera[0..2]` is an angle-axis rotation (which represents $R$).\n",
    "- `camera[3..5]` is a translation vector (which represents $\\mathbf{t}$).\n",
    "- `camera[6]` is the focal length, and `camera[7..8]` are radial distortion coefficients.\n",
    "\n",
    "The code implements:\n",
    "$\n",
    "\\mathbf{p}' = R \\,\\mathbf{X} + \\mathbf{t}\n",
    "$\n",
    "where:\n",
    "- $\\mathbf{X}$ is the 3D point (in *world* coordinates),\n",
    "- $R$ and $\\mathbf{t}$ transform that point *from world* into *the camera’s frame*,\n",
    "- Then it projects to 2D (accounting for the Bundler sign convention on the z-axis and radial distortion).\n",
    "\n",
    "Hence, the expectation is that `point` (the 3D point) lives in the *same consistent world coordinate system* that corresponds to each camera’s extrinsics $(R,\\mathbf{t})$.  \n",
    "\n",
    "If each newly triangulated set of 3D points is instead stored in the local coordinate system of whichever camera you used to triangulate them, then the above transform $R\\mathbf{X} + \\mathbf{t}$ will not be consistent, because $\\mathbf{X}$ is no longer the same “world” $\\mathbf{X}$.  Therefore you must transform everything into *one* frame—usually picking the first camera as the “world” frame.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Typical Incremental SfM Workflow\n",
    "\n",
    "1. **Choose a reference** (often the first camera) to define the origin $(R=I, \\mathbf{t}=\\mathbf{0})$.  \n",
    "2. **Add a second camera**:\n",
    "   - You find $\\mathbf{R}_{1\\to2}$ and $\\mathbf{t}_{1\\to2}$ by `cv::recoverPose` (which gives the transformation from camera 1 to camera 2).  \n",
    "   - You convert that to a pose $(R_{2}, t_{2})$ in the “world” frame, which is simply $\\mathbf{R}_2 = \\mathbf{R}_{1\\to2}$ and $\\mathbf{t}_2 = \\mathbf{t}_{1\\to2}$ if you treat camera 1 as the origin.  \n",
    "   - Triangulate points in the reference frame (camera 1’s frame). In OpenCV, typically you use something like:\n",
    "     ```cpp\n",
    "     triangulatePoints(P1, P2, matched_points1, matched_points2, points4D);\n",
    "     ```\n",
    "     Here, `P1` could be `[I|0]`, and `P2` is `[R_{1->2} | t_{1->2}]`, so the resulting 3D points are effectively in camera 1’s coordinate system (depending on your usage of `triangulatePoints`).\n",
    "   - Now you have 3D points (in world = camera 1’s frame) plus camera 2’s extrinsics in that frame.\n",
    "\n",
    "3. **Add a third camera**:\n",
    "   - You match keypoints between camera 2 and camera 3, and do another `recoverPose` to get $\\mathbf{R}_{2\\to3}$ and $\\mathbf{t}_{2\\to3}$.  \n",
    "   - **But** in order to maintain *everything* in camera 1’s world frame, you must convert this camera-3 pose to the same global reference frame.  \n",
    "     $\n",
    "       \\mathbf{R}_{1\\to3} = \\mathbf{R}_{1\\to2} \\,\\mathbf{R}_{2\\to3}, \n",
    "       \\quad\n",
    "       \\mathbf{t}_{1\\to3} = \\mathbf{R}_{1\\to2}\\,\\mathbf{t}_{2\\to3} + \\mathbf{t}_{1\\to2}.\n",
    "     $\n",
    "   - Once you have $\\mathbf{R}_{1\\to3}$ and $\\mathbf{t}_{1\\to3}$, those are the extrinsic parameters for camera 3 *in the same world frame*.  \n",
    "   - If you triangulate new points with camera 2 and camera 3, those new points will initially end up in camera-2’s frame (depending on how you call `triangulatePoints`).  So you must transform them from camera 2’s frame into camera 1’s frame:\n",
    "     $\n",
    "       \\mathbf{X}_{w} \\;=\\; \\mathbf{R}_{1\\to2}\\, \\mathbf{X}_{2} \\;+\\; \\mathbf{t}_{1\\to2}.\n",
    "     $\n",
    "     (Or whichever formula matches how your P-matrices are set up in OpenCV.  The key is to bring $\\mathbf{X}$ into the same “world” coordinate system.)\n",
    "\n",
    "4. **Add all cameras and all points** (now in a common frame) to the bundle adjuster.  Each camera’s extrinsics are parameterized by an angle-axis and translation that map *world $\\to$ that camera*, and each 3D point $\\mathbf{X}_w$ is stored in the same world coordinate system.  \n",
    "5. **Refine** all parameters via bundle adjustment.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Do You *Have* to Transform Points and Poses Every Time?\n",
    "\n",
    "- **Yes, if** you want to keep a single global frame (which is by far the most common approach), you must express any newly triangulated points and newly computed camera extrinsics in that same frame.  \n",
    "- You do *not* necessarily have to re-triangulate *old* points if they are already consistent.  You only have to transform:\n",
    "  1. The newly found camera pose into the global frame,  \n",
    "  2. The newly triangulated 3D points into the global frame,  \n",
    "  3. Then add them to the optimization problem.\n",
    "\n",
    "Once everything is in one consistent coordinate system, the standard Snavely reprojection model works directly.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "- Pick camera 1 as *world* with $\\mathbf{R}=I$, $\\mathbf{t}=\\mathbf{0}$.  \n",
    "- For each new camera $k$:\n",
    "  1. Compute $\\mathbf{R}_{k}$ and $\\mathbf{t}_{k}$ **in the world frame** (using the known pose from a previous camera that is already in the world frame).  \n",
    "  2. Triangulate new points (they might initially come out in camera $k$’s frame or camera j’s frame).  Convert them to *world* coordinates.  \n",
    "  3. Add $\\mathbf{R}_{k}, \\mathbf{t}_{k}$ and the 3D points in *world* frame to the bundle adjuster.\n",
    "\n",
    "This ensures that each residual evaluation inside `SnavelyReprojectionError` does exactly:\n",
    "$\n",
    "\\mathbf{p}_{\\text{cam}} \n",
    "= R_{k} \\,\\mathbf{X}_{w} \\;+\\; \\mathbf{t}_{k}\n",
    "$\n",
    "for the 3D point $\\mathbf{X}_w$ (in world coordinates) and camera $k$’s extrinsics $\\{R_{k}, \\mathbf{t}_{k}\\}$.  No confusion about local frames is necessary once everything is transformed consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beeebc7-2f25-47f1-b52c-4d03be5c422a",
   "metadata": {},
   "source": [
    "**Short Answer**  \n",
    "Yes, if you have 3D points expressed in camera 2’s frame (after triangulating between camera 2 and camera 3), and you want those points in the **world** frame (which you’ve designated as camera 1’s frame), you need to apply the **inverse** of the transform that takes points from camera 1 to camera 2. In other words, if your `recoverPose` gave you $R_{1\\to2}$ and $\\mathbf{t}_{1\\to2}$ (so that $\\mathbf{X}_2 = R_{1\\to2}\\,\\mathbf{X}_1 + \\mathbf{t}_{1\\to2}$), then to go from camera 2 coordinates back to camera 1 coordinates (“world”), you must invert that:\n",
    "$\n",
    "\\mathbf{X}_1 \n",
    "= \n",
    "R_{1\\to2}^{\\,\\top}\n",
    "\\bigl(\\mathbf{X}_2 - \\mathbf{t}_{1\\to2}\\bigr).\n",
    "$\n",
    "Equivalently,\n",
    "$\n",
    "R_{2\\to1} = R_{1\\to2}^{\\,\\top}, \n",
    "\\quad\n",
    "\\mathbf{t}_{2\\to1} = -\\,R_{1\\to2}^{\\,\\top} \\,\\mathbf{t}_{1\\to2}.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Explanation\n",
    "\n",
    "1. **What `recoverPose` Returns**\n",
    "\n",
    "   When you call  \n",
    "   ```cpp\n",
    "   recoverPose(E, points1, points2, K, R, t);\n",
    "   ```  \n",
    "   with $`points1` = camera 1, `points2` = camera 2$,\n",
    "   - It returns $R$ and $t$ such that a 3D point $\\mathbf{X}_1$ in camera 1’s coordinates is transformed to camera 2’s coordinates by:  \n",
    "     $\n",
    "       \\mathbf{X}_2 \\;=\\; R\\,\\mathbf{X}_1 \\;+\\; t.\n",
    "     $\n",
    "   - We can denote that as $\\,^{2}T_{1} = (R_{1\\to2},\\, t_{1\\to2})$.\n",
    "\n",
    "2. **Camera 1 as World**\n",
    "\n",
    "   In many SfM pipelines, we choose camera 1’s coordinate system to be the “world” frame. Then camera 2’s extrinsic parameters $(R_2,\\, t_2)$ are exactly $(R_{1\\to2},\\, t_{1\\to2})$, meaning:\n",
    "   $\n",
    "     \\mathbf{X}_2 \n",
    "     \\;=\\;\n",
    "     R_2\\,\\mathbf{X}_\\mathrm{world} + t_2\n",
    "     \\;\\;(\\text{but here $\\mathbf{X}_\\mathrm{world}\\equiv \\mathbf{X}_1$}).\n",
    "   $\n",
    "   No inversion is needed to define camera 2’s pose with respect to the world.\n",
    "\n",
    "3. **Triangulating Between Camera 2 and Camera 3**\n",
    "\n",
    "   - Suppose you then add camera 3. You match features between camera 2 and camera 3 and do `recoverPose` again to get $R_{2\\to3}$, $\\mathbf{t}_{2\\to3}$. You might use OpenCV’s `triangulatePoints` with projection matrices that place camera 2 at the origin of that triangulation process.\n",
    "   - The resulting 3D points from that call will come out in **camera 2’s** local coordinate system (depending on how you formed your projection matrices `P2`, `P3`).\n",
    "\n",
    "4. **Convert Those Points to the World Frame (camera 1)**\n",
    "\n",
    "   Now you have some new 3D points $\\mathbf{X}_2$ in camera 2’s frame, but you want everything in the same world (camera 1) coordinate system. The question is: “How do I go from camera 2 coords to camera 1 coords?”\n",
    "\n",
    "   - If we know \n",
    "     $\n",
    "       \\mathbf{X}_2 \n",
    "       \\;=\\;\n",
    "       R_{1\\to2}\\,\\mathbf{X}_1 \n",
    "       + \n",
    "       \\mathbf{t}_{1\\to2},\n",
    "     $\n",
    "     then we invert it:\n",
    "     $\n",
    "       \\mathbf{X}_1 \n",
    "       \\;=\\;\n",
    "       R_{1\\to2}^{\\,\\top}\n",
    "       \\bigl(\\mathbf{X}_2 - \\mathbf{t}_{1\\to2}\\bigr).\n",
    "     $\n",
    "   - Or equivalently, define \n",
    "     $\n",
    "       R_{2\\to1} \n",
    "       = \n",
    "       R_{1\\to2}^{\\,\\top}\n",
    "       , \n",
    "       \\quad\n",
    "       \\mathbf{t}_{2\\to1}\n",
    "       =\n",
    "       -\\,R_{1\\to2}^{\\,\\top} \\,\\mathbf{t}_{1\\to2},\n",
    "     $\n",
    "     so \n",
    "     $\n",
    "       \\mathbf{X}_1 \n",
    "       =\n",
    "       R_{2\\to1}\\,\\mathbf{X}_2\n",
    "       +\n",
    "       \\mathbf{t}_{2\\to1}.\n",
    "     $\n",
    "\n",
    "5. **Bundle Adjustment**\n",
    "\n",
    "   Once you have the newly triangulated points in the world (camera 1) coordinates, you can add them (and camera 3’s pose in world coords) into your bundle adjuster. During bundle adjustment:\n",
    "   - Each 3D point is stored in world coords,\n",
    "   - Each camera has extrinsics that transform a world point $\\mathbf{X}_\\mathrm{w}$ into that camera’s local frame,\n",
    "   $\n",
    "     \\mathbf{X}_\\mathrm{cam} = R_\\mathrm{cam}\\,\\mathbf{X}_\\mathrm{w} + t_\\mathrm{cam}.\n",
    "   $\n",
    "\n",
    "Thus, **yes**—if your triangulation code yields points in camera 2’s reference frame, and you want them in camera 1’s (world) frame, you use the **inverse** of the “camera 1 to camera 2” transform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
