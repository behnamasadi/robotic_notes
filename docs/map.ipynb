{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51688876-96c7-408a-85cc-244de9753074",
   "metadata": {},
   "source": [
    "##  Multivariate Gaussian PDF \n",
    "\n",
    "For a random vector $y \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "p(y) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2}\n",
    "(y-\\mu)^T\n",
    "\\Sigma^{-1}\n",
    "(y-\\mu)\n",
    "\\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "This is the **template**.\n",
    "Everything else is just **choosing what $y$ and $\\mu$ represent**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Prior $p(x)$ â€” Gaussian over the state\n",
    "\n",
    "Here, the **random variable is the state itself**.\n",
    "\n",
    "Assume:\n",
    "\n",
    "$$\n",
    "x \\sim \\mathcal{N}(\\mu_x, \\Sigma_x)\n",
    "$$\n",
    "\n",
    "Then the PDF is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "p(x) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma_x|}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2}\n",
    "(x-\\mu_x)^T\n",
    "\\Sigma_x^{-1}\n",
    "(x-\\mu_x)\n",
    "\\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "### Meaning\n",
    "\n",
    "* $x$ is random\n",
    "* $\\mu_x$ is your **prior belief**\n",
    "* $\\Sigma_x$ is **how uncertain** that belief is\n",
    "\n",
    "---\n",
    "\n",
    "## Likelihood $p(z \\mid x)$ â€” Gaussian over the measurement\n",
    "\n",
    "Here, the **random variable is the measurement**, **conditioned on a fixed $x$**.\n",
    "\n",
    "Measurement model:\n",
    "\n",
    "$$\n",
    "z = h(x) + v, \\quad v \\sim \\mathcal{N}(0,\\Sigma_z)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "z \\mid x \\sim \\mathcal{N}(h(x),\\Sigma_z)\n",
    "$$\n",
    "\n",
    "The likelihood PDF is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "p(z \\mid x) = \\frac{1}{\\sqrt{(2\\pi)^m |\\Sigma_z|}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2}\n",
    "(z-h(x))^T\n",
    "\\Sigma_z^{-1}\n",
    "(z-h(x))\n",
    "\\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "### Meaning\n",
    "\n",
    "* $z$ is random\n",
    "* $x$ is treated as **given**\n",
    "* The mean depends on $x$ via $h(x)$\n",
    "\n",
    "---\n",
    "\n",
    "##  Side-by-side comparison (this removes the confusion)\n",
    "\n",
    "| Aspect          | Prior $p(x)$       | Likelihood $p(z\\mid x)$ |\n",
    "| --------------- | ------------------ | ----------------------- |\n",
    "| Random variable | $x$                | $z$                     |\n",
    "| Mean            | $\\mu_x$            | $h(x)$                  |\n",
    "| Covariance      | $\\Sigma_x$         | $\\Sigma_z$              |\n",
    "| Depends on $x$? | Yes (directly)     | Yes (through $h(x)$)    |\n",
    "| What it encodes | belief before data | sensor model            |\n",
    "\n",
    "---\n",
    "\n",
    "##  Key subtle but crucial point\n",
    "\n",
    "When doing **MAP optimization**:\n",
    "\n",
    "* In $p(x)$, **$x$ is the variable**\n",
    "* In $p(z \\mid x)$, **$z$ is fixed** and **$x$ appears inside the mean**\n",
    "\n",
    "So for optimization:\n",
    "\n",
    "$$\n",
    "p(z \\mid x) \\text{ is viewed as a function of } x\n",
    "$$\n",
    "\n",
    "even though it is formally a PDF over $z$.\n",
    "\n",
    "---\n",
    "\n",
    "##  Negative log (why they look identical in least squares)\n",
    "\n",
    "### Prior term\n",
    "\n",
    "$$\n",
    "-\\log p(x) = \\frac{1}{2}\n",
    "(x-\\mu_x)^T\n",
    "\\Sigma_x^{-1}\n",
    "(x-\\mu_x)\n",
    "+ \\text{const}\n",
    "$$\n",
    "\n",
    "### Likelihood term\n",
    "\n",
    "$$\n",
    "-\\log p(z \\mid x) = \\frac{1}{2}\n",
    "(z-h(x))^T\n",
    "\\Sigma_z^{-1}\n",
    "(z-h(x))\n",
    "+ \\text{const}\n",
    "$$\n",
    "\n",
    " Same structure, **different semantics**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde9480-ccdc-440c-bfbd-9a9c9d51dbd0",
   "metadata": {},
   "source": [
    "# Maximum A-Posteriori (MAP) Estimation\n",
    "**MAP** answers this question:\n",
    "\n",
    "> Given some noisy measurements, what is the **most likely value of the unknown variable**, taking into account\n",
    "- âœ… what we already believed before (prior)\n",
    "- âœ… how measurements are generated (likelihood)\n",
    "\n",
    "Formally:\n",
    "\n",
    "> **MAP = the value of the state that maximizes the posterior probability**\n",
    "\n",
    "---\n",
    "\n",
    "##  The MAP formula\n",
    "\n",
    "We want the state $x$ that maximizes the posterior:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^{\\ast} = \\arg\\max_x \\; p(x \\mid z)\n",
    "}\n",
    "$$\n",
    "\n",
    "Using **Bayes' rule**:\n",
    "\n",
    "$$\n",
    "p(x \\mid z) = \\frac{p(z \\mid x),p(x)}{p(z)}\n",
    "$$\n",
    "\n",
    "Since $p(z)$ does **not depend on $x$**, we can drop it:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^{\\ast} = \\arg\\max_x \\; p(z \\mid x),p(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Why MAP is used in optimization\n",
    "\n",
    "We usually **minimize the negative log** instead of maximizing probabilities:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^{\\ast} = \\arg\\min_x \\Big(\n",
    "-\\log p(z \\mid x) - \\log p(x)\n",
    "\\Big)\n",
    "}\n",
    "$$\n",
    "\n",
    "This becomes a **least-squares problem** when noises are Gaussian.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "##  Simple 1D numeric example\n",
    "\n",
    "### Problem setup\n",
    "\n",
    "You want to estimate a **1D position $x$**\n",
    "\n",
    "#### Prior belief\n",
    "\n",
    "You believe:\n",
    "\n",
    "$$\n",
    "x \\sim \\mathcal{N}(2, 1)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* Mean $=2$\n",
    "* Variance $=1$\n",
    "\n",
    "#### Measurement\n",
    "\n",
    "You measure:\n",
    "\n",
    "$$\n",
    "z = 4\n",
    "$$\n",
    "\n",
    "\n",
    "What does $z = 4$ mean in that example?\n",
    "\n",
    "**The sensor says the position is 4**\n",
    "\n",
    "But more precisely:\n",
    "\n",
    "* $x$ = **true but unknown position**\n",
    "* $z$ = **measured value reported by the sensor**\n",
    "* The sensor is **noisy**\n",
    "\n",
    "So the measurement model is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "z = x + v\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "v \\sim \\mathcal{N}(0,\\sigma^2)\n",
    "$$\n",
    "\n",
    "In the example, $\\sigma^2 = 1$.\n",
    "\n",
    "So when we say:\n",
    "\n",
    "$$\n",
    "z = 4\n",
    "$$\n",
    "\n",
    "it means:\n",
    "\n",
    "â€œThe sensor output is 4, but the true position may not be exactly 4.â€\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Measurement noise:\n",
    "\n",
    "$$\n",
    "z = x + v, \\quad v \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Write likelihood and prior\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "$$\n",
    "p(z \\mid x)\n",
    "\\propto\n",
    "\\exp\n",
    "\\Big(\n",
    "-\\frac{1}{2}(z - x)^2\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "### Prior\n",
    "\n",
    "$$\n",
    "p(x)\n",
    "\\propto\n",
    "\\exp\n",
    "\\Big(\n",
    "-\\frac{1}{2}(x - 2)^2\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  MAP cost function\n",
    "\n",
    "Multiply and take negative log:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "J(x) = \\frac{1}{2}(z - x)^2 + \\frac{1}{2}(x - 2)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "Substitute $z = 4$:\n",
    "\n",
    "$$\n",
    "J(x) = \\frac{1}{2}(4 - x)^2 + \\frac{1}{2}(x - 2)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Minimize the cost\n",
    "\n",
    "Take derivative and set to zero:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dx} = -(4 - x) + (x - 2) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "-4 + x + x - 2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "2x = 6\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^{\\ast} = 3\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Interpretation\n",
    "\n",
    "| Source       | Suggests |\n",
    "| ------------ | -------- |\n",
    "| Prior        | $x=2$    |\n",
    "| Measurement  | $x=4$    |\n",
    "| MAP estimate | $x=3$    |\n",
    "\n",
    "ðŸ‘‰ MAP **balances trust** between prior and measurement.\n",
    "\n",
    "---\n",
    "\n",
    "##  Connection to SLAM / factor graphs\n",
    "\n",
    "MAP in SLAM looks like:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\min_{\\mathcal{X}}\n",
    "\\sum_f | r_f(\\mathcal{X}_f) |_{\\Omega_f}^2\n",
    "}\n",
    "$$\n",
    "\n",
    "* Each **factor** = likelihood or prior\n",
    "* Each **information matrix $\\Omega_f$** = confidence\n",
    "* This is **exactly MAP estimation**\n",
    "\n",
    "---\n",
    "\n",
    "##  One-line intuition\n",
    "\n",
    "> **MAP = \"the most likely state, given what I knew before and what I just observed.\"**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02de76-eef4-438f-a721-5aac8c4a6c54",
   "metadata": {},
   "source": [
    "##  Likelihood explained intuitively\n",
    "\n",
    "### Measurement model\n",
    "\n",
    "$$\n",
    "z = h(x) + v\n",
    "$$\n",
    "\n",
    "For this example:\n",
    "\n",
    "$$\n",
    "h(x) = x\n",
    "$$\n",
    "\n",
    "Noise:\n",
    "\n",
    "$$\n",
    "v \\sim \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood distribution\n",
    "\n",
    "Given a **fixed $x$**, the measurement $z$ is random:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "p(z \\mid x) = \\mathcal{N}(x,1)\n",
    "}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "* If $x=4$, sensor is **likely** to report values near 4\n",
    "* If $x=2$, sensor is **unlikely** to report 4\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood function (as a function of $x$)\n",
    "\n",
    "Now we **fix the actual measurement** $z=4$\n",
    "and view $p(z \\mid x)$ as a function of $x$:\n",
    "\n",
    "$$\n",
    "p(z=4 \\mid x)\n",
    "\\propto\n",
    "\\exp\n",
    "\\Big(\n",
    "-\\frac{1}{2}(4 - x)^2\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "| $x$   | Likelihood |\n",
    "| ----- | ---------- |\n",
    "| $x=4$ | very high  |\n",
    "| $x=3$ | high       |\n",
    "| $x=2$ | lower      |\n",
    "| $x=0$ | very low   |\n",
    "\n",
    "This curve is centered at **$x=4$**.\n",
    "\n",
    "---\n",
    "\n",
    "##  What is the Prior $p(x)$?\n",
    "\n",
    "The **prior** encodes **what you believe before seeing the measurement**.\n",
    "\n",
    "Example belief:\n",
    "\n",
    "> \"Before measuring anything, I think the position is around 2 Â± 1\"\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "p(x) = \\mathcal{N}(2,1)\n",
    "}\n",
    "$$\n",
    "\n",
    "This might come from:\n",
    "\n",
    "* previous time step\n",
    "* odometry\n",
    "* motion model\n",
    "* GPS history\n",
    "* initial guess\n",
    "\n",
    "---\n",
    "\n",
    "### Prior as a cost\n",
    "\n",
    "Negative log-prior:\n",
    "\n",
    "$$\n",
    "-\\log p(x) = \\frac{1}{2}(x-2)^2 + \\text{const}\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "\n",
    "> Deviating from 2 is penalized\n",
    "\n",
    "---\n",
    "\n",
    "##  Combine Likelihood + Prior = Posterior\n",
    "\n",
    "Bayes:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "p(x \\mid z) \\propto p(z \\mid x),p(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "Substitute:\n",
    "\n",
    "$$ p(x \\mid z) \\propto \\exp \\Big( -\\frac{1}{2}(4 - x)^2 - \\frac{1}{2}(x - 2)^2 \\Big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  MAP = maximize posterior = minimize cost\n",
    "\n",
    "Negative log:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "J(x) = \\frac{1}{2}(4 - x)^2 + \\frac{1}{2}(x - 2)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "This cost has **two competing forces**:\n",
    "\n",
    "| Term      | Meaning              |\n",
    "| --------- | -------------------- |\n",
    "| $(4-x)^2$ | stay close to sensor |\n",
    "| $(x-2)^2$ | stay close to prior  |\n",
    "\n",
    "---\n",
    "\n",
    "##  Why the answer is $x^\\ast=3$\n",
    "\n",
    "* Sensor pulls estimate toward 4\n",
    "* Prior pulls estimate toward 2\n",
    "* Equal confidence â†’ midpoint\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^\\ast = 3\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Physical interpretation (SLAM analogy)\n",
    "\n",
    "| SLAM term          | This example    |\n",
    "| ------------------ | --------------- |\n",
    "| Node state         | $x$             |\n",
    "| Measurement        | $z=4$           |\n",
    "| Measurement factor | $p(z \\mid x)$   |\n",
    "| Prior factor       | $p(x)$          |\n",
    "| Information matrix | $\\Sigma^{-1}=1$ |\n",
    "\n",
    "---\n",
    "\n",
    "##  One-sentence takeaway\n",
    "\n",
    "> $z=4$ means \"the sensor says 4,\"\n",
    "> the likelihood says \"how plausible that is for each possible $x$,\"\n",
    "> and the prior says \"what I believed before measuring.\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623aca8-e87b-462f-aa2b-30b93e448c5d",
   "metadata": {},
   "source": [
    "##  Starting point: MAP definition\n",
    "\n",
    "MAP seeks\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^{\\ast} = \\arg\\max_x \\; p(x \\mid z)\n",
    "}\n",
    "$$\n",
    "\n",
    "Using Bayes:\n",
    "\n",
    "$$\n",
    "p(x \\mid z) = \\frac{p(z \\mid x),p(x)}{p(z)}\n",
    "$$\n",
    "\n",
    "Since $p(z)$ does **not depend on $x$**:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^{\\ast} = \\arg\\max_x \\; p(z \\mid x),p(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Problem with maximizing probabilities directly\n",
    "\n",
    "###  Numerical issues\n",
    "\n",
    "Probabilities are:\n",
    "\n",
    "* very small numbers\n",
    "* products of many terms\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "p(z \\mid x) = 10^{-50}, \\quad p(x)=10^{-30}\n",
    "$$\n",
    "\n",
    "Product:\n",
    "\n",
    "$$\n",
    "p(z \\mid x)p(x)=10^{-80}\n",
    "$$\n",
    "\n",
    "This causes **floating-point underflow**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Apply the logarithm\n",
    "\n",
    "Logarithm has two key properties:\n",
    "\n",
    "### Property 1 â€” monotonicity\n",
    "\n",
    "$$\n",
    "\\arg\\max_x f(x) = \\arg\\max_x \\log f(x)\n",
    "$$\n",
    "\n",
    "Log does **not change the maximizer**.\n",
    "\n",
    "### Property 2 â€” product â†’ sum\n",
    "\n",
    "$$\n",
    "\\log(ab) = \\log a + \\log b\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Apply log to MAP objective\n",
    "\n",
    "$$\n",
    "x^{\\ast} = \\arg\\max_x \\Big(\n",
    "\\log p(z \\mid x) + \\log p(x)\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Convert maximization to minimization\n",
    "\n",
    "Optimizers usually **minimize**, so multiply by $-1$:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x^{\\ast} = \\arg\\min_x \\Big(\n",
    "-\\log p(z \\mid x) - \\log p(x)\n",
    "\\Big)\n",
    "}\n",
    "$$\n",
    "\n",
    "This is called the **negative log-posterior**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why this becomes least-squares (Gaussian case)\n",
    "\n",
    "Assume Gaussian noise:\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "$$\n",
    "p(z \\mid x) = \\mathcal{N}(h(x), \\Sigma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(z \\mid x)\n",
    "\\propto\n",
    "\\exp\n",
    "\\Big(\n",
    "-\\frac{1}{2}\n",
    "(z - h(x))^T\n",
    "\\Sigma^{-1}\n",
    "(z - h(x))\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "Take negative log:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "-\\log p(z \\mid x) = \\frac{1}{2}\n",
    "(z - h(x))^T\n",
    "\\Sigma^{-1}\n",
    "(z - h(x))\n",
    "+ \\text{const}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Prior\n",
    "\n",
    "$$\n",
    "p(x) = \\mathcal{N}(\\mu, \\Sigma_0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "-\\log p(x) = \\frac{1}{2}\n",
    "(x - \\mu)^T\n",
    "\\Sigma_0^{-1}\n",
    "(x - \\mu)\n",
    "+ \\text{const}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Final MAP optimization problem\n",
    "\n",
    "Dropping constants:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\min_x\n",
    "\\Big[\n",
    "\\frac{1}{2}\n",
    "| z - h(x) |_{\\Sigma^{-1}}^2\n",
    "+\n",
    "\\frac{1}{2}\n",
    "| x - \\mu |_{\\Sigma_0^{-1}}^2\n",
    "\\Big]\n",
    "}\n",
    "$$\n",
    "\n",
    "This is exactly a **weighted least-squares problem**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why this is perfect for SLAM / VIO\n",
    "\n",
    "* Each residual term = one measurement\n",
    "* Each weight $\\Sigma^{-1}$ = information matrix\n",
    "* Sum of squared residuals = MAP estimate\n",
    "\n",
    "This leads directly to:\n",
    "\n",
    "$$\n",
    "H\\Delta x = b\n",
    "$$\n",
    "\n",
    "after linearization.\n",
    "\n",
    "---\n",
    "\n",
    "##  Intuition in one sentence\n",
    "\n",
    "Taking the **negative log** turns an impossible-to-optimize product of tiny probabilities into a **stable sum of squared errors** that standard solvers can minimize.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29440751-e62e-46fd-b06c-973b1e5e5259",
   "metadata": {},
   "source": [
    "\n",
    "##  Start from the measurement model\n",
    "\n",
    "We assume a **Gaussian sensor model**:\n",
    "\n",
    "$$\n",
    "z = h(x) + v\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $x$ = unknown state\n",
    "* $h(x)$ = predicted measurement from the model\n",
    "* $v \\sim \\mathcal{N}(0, \\Sigma)$ = Gaussian noise\n",
    "\n",
    "This means:\n",
    "\n",
    "> If the true state is $x$, then measurements are distributed around $h(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "##  Write the likelihood explicitly\n",
    "\n",
    "The probability density of a multivariate Gaussian is:\n",
    "\n",
    "$$\n",
    "p(z \\mid x) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2}\n",
    "(z - h(x))^T\n",
    "\\Sigma^{-1}\n",
    "(z - h(x))\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Key observation:\n",
    "\n",
    "* The **only part depending on $x$** is the quadratic term\n",
    "* The front constant does **not affect optimization**\n",
    "\n",
    "---\n",
    "\n",
    "##  Take the negative logarithm\n",
    "\n",
    "MAP minimizes the **negative log-posterior**, so we compute:\n",
    "\n",
    "$$\n",
    "-\\log p(z \\mid x)\n",
    "$$\n",
    "\n",
    "Apply log:\n",
    "\n",
    "$$\n",
    "-\\log p(z \\mid x) = \\frac{1}{2}\n",
    "(z - h(x))^T\n",
    "\\Sigma^{-1}\n",
    "(z - h(x))\n",
    "+\n",
    "\\frac{1}{2}\\log |\\Sigma|\n",
    "+\n",
    "\\frac{n}{2}\\log(2\\pi)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Drop constants (critical step)\n",
    "\n",
    "The last two terms:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\log |\\Sigma|\n",
    "\\quad,\\quad\n",
    "\\frac{n}{2}\\log(2\\pi)\n",
    "$$\n",
    "\n",
    "* do **not depend on $x$**\n",
    "* therefore do **not change the minimizer**\n",
    "\n",
    "So we discard them.\n",
    "\n",
    "---\n",
    "\n",
    "##  What remains (this is the key result)\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "-\\log p(z \\mid x) = \\frac{1}{2}\n",
    "(z - h(x))^T\n",
    "\\Sigma^{-1}\n",
    "(z - h(x))\n",
    "}\n",
    "$$\n",
    "\n",
    "This is a **quadratic form** in the residual:\n",
    "\n",
    "$$\n",
    "r(x) = z - h(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Why this is called \"least squares\"\n",
    "\n",
    "Define the **weighted norm**:\n",
    "\n",
    "$$\n",
    "| r |_{\\Sigma^{-1}}^2 = r^T \\Sigma^{-1} r\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "-\\log p(z \\mid x) = \\frac{1}{2}\n",
    "| z - h(x) |_{\\Sigma^{-1}}^2\n",
    "}\n",
    "$$\n",
    "\n",
    "This is exactly a **weighted least-squares cost**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Intuition behind the weighting\n",
    "\n",
    "### Scalar case\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\sigma^2\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\Sigma^{-1} = \\frac{1}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "So the cost becomes:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\n",
    "\\frac{(z - h(x))^2}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Small variance â†’ strong penalty\n",
    "* Large variance â†’ weak penalty\n",
    "\n",
    "This is **why the information matrix is the inverse covariance**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Extend to multiple measurements\n",
    "\n",
    "For independent measurements $z_i$:\n",
    "\n",
    "$$\n",
    "p(\\{z_i\\} \\mid x) = \\prod_i p(z_i \\mid x)\n",
    "$$\n",
    "\n",
    "Negative log turns product into sum:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\sum_i\n",
    "\\frac{1}{2}\n",
    "| z_i - h_i(x) |_{\\Sigma_i^{-1}}^2\n",
    "}\n",
    "$$\n",
    "\n",
    "This is the standard **sum-of-squared residuals** form.\n",
    "\n",
    "---\n",
    "\n",
    "##  Add the prior (completing MAP)\n",
    "\n",
    "If the prior is Gaussian:\n",
    "\n",
    "$$\n",
    "x \\sim \\mathcal{N}(\\mu, \\Sigma_0)\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "-\\log p(x) = \\frac{1}{2}\n",
    "(x - \\mu)^T\n",
    "\\Sigma_0^{-1}\n",
    "(x - \\mu)\n",
    "$$\n",
    "\n",
    "Final MAP problem:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\min_x\n",
    "\\left[\n",
    "\\sum_i\n",
    "\\frac{1}{2}\n",
    "| z_i - h_i(x) |_{\\Sigma_i^{-1}}^2\n",
    "+\n",
    "\\frac{1}{2}\n",
    "| x - \\mu |_{\\Sigma_0^{-1}}^2\n",
    "\\right]\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Why this matters in practice\n",
    "\n",
    "This exact derivation explains why:\n",
    "\n",
    "* SLAM is least squares\n",
    "* Bundle adjustment is least squares\n",
    "* Kalman filters minimize quadratic costs\n",
    "* Factor graphs store $\\Sigma^{-1}$ (information)\n",
    "\n",
    "No heuristics â€” it **falls straight out of Gaussian probability theory**.\n",
    "\n",
    "---\n",
    "\n",
    "## One-sentence takeaway\n",
    "\n",
    "Gaussian noise + MAP estimation **inevitably** leads to weighted least-squares optimization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d6302-9eee-4ba8-9736-a9068e9154d0",
   "metadata": {},
   "source": [
    "##  MAP vs ML (important distinction)\n",
    "\n",
    "| Method                     | Uses prior? | Meaning              |\n",
    "| -------------------------- | ----------- | -------------------- |\n",
    "| Maximum Likelihood (ML)    | âŒ No        | Trust only the data  |\n",
    "| Maximum A-Posteriori (MAP) | âœ… Yes       | Balance prior + data |\n",
    "\n",
    "ML is a **special case of MAP** when the prior is uniform.\n",
    "\n",
    "\n",
    "\n",
    "##  Definitions side by side\n",
    "\n",
    "### Maximum Likelihood (ML)\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x_{\\text{ML}} = \\arg\\max_x \\; p(z \\mid x)\n",
    "}\n",
    "$$\n",
    "\n",
    "Only the **likelihood** appears.\n",
    "\n",
    "---\n",
    "\n",
    "### Maximum A-Posteriori (MAP)\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x_{\\text{MAP}} = \\arg\\max_x \\; p(z \\mid x),p(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "Likelihood **plus prior**.\n",
    "\n",
    "---\n",
    "\n",
    "##  What is *physically* different?\n",
    "\n",
    "| Quantity          | ML      | MAP      |\n",
    "| ----------------- | ------- | -------- |\n",
    "| Measurement model | âœ…       | âœ…        |\n",
    "| Sensor noise      | âœ…       | âœ…        |\n",
    "| Prior knowledge   | âŒ       | âœ…        |\n",
    "| Initial belief    | ignored | enforced |\n",
    "| Drift control     | âŒ       | âœ…        |\n",
    "\n",
    "---\n",
    "\n",
    "##  Same measurement, two different estimators\n",
    "\n",
    "### Measurement model\n",
    "\n",
    "$$\n",
    "z = x + v,\\quad v \\sim \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "Measurement:\n",
    "\n",
    "$$\n",
    "z = 4\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  ML solution (no prior)\n",
    "\n",
    "Likelihood:\n",
    "\n",
    "$$\n",
    "p(z \\mid x)\n",
    "\\propto\n",
    "\\exp\n",
    "\\Big(\n",
    "-\\frac{1}{2}(4-x)^2\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "ML maximizes this, so:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x_{\\text{ML}} = 4\n",
    "}\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "> \"I fully trust the sensor.\"\n",
    "\n",
    "---\n",
    "\n",
    "##  MAP solution (with prior)\n",
    "\n",
    "Add a prior:\n",
    "\n",
    "$$\n",
    "x \\sim \\mathcal{N}(2,1)\n",
    "$$\n",
    "\n",
    "MAP cost:\n",
    "\n",
    "$$\n",
    "J(x) = \\frac{1}{2}(4-x)^2 + \\frac{1}{2}(x-2)^2\n",
    "$$\n",
    "\n",
    "Minimizing:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x_{\\text{MAP}} = 3\n",
    "}\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "> \"I compromise between belief and measurement.\"\n",
    "\n",
    "---\n",
    "\n",
    "##  Why ML is a special case of MAP\n",
    "\n",
    "If the prior is **uniform**:\n",
    "\n",
    "$$\n",
    "p(x) = \\text{const}\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "p(z \\mid x),p(x) \\propto p(z \\mid x)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{ML} = \\text{MAP with flat prior}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Least-squares view (important for SLAM)\n",
    "\n",
    "### ML objective\n",
    "\n",
    "$$\n",
    "\\min_x \\frac{1}{2} | z - h(x) |_{\\Sigma^{-1}}^2\n",
    "$$\n",
    "\n",
    "Only **measurement residuals**.\n",
    "\n",
    "---\n",
    "\n",
    "### MAP objective\n",
    "\n",
    "$$\n",
    "\\min_x\n",
    "\\left[\n",
    "\\frac{1}{2} | z - h(x) |_{\\Sigma^{-1}}^2\n",
    "+\n",
    "\\frac{1}{2} | x - \\mu |_{\\Sigma_0^{-1}}^2\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Measurement + **regularization from prior**.\n",
    "\n",
    "---\n",
    "\n",
    "##  SLAM interpretation (very important)\n",
    "\n",
    "| SLAM term     | ML          | MAP          |\n",
    "| ------------- | ----------- | ------------ |\n",
    "| Odometry      | measurement | measurement  |\n",
    "| Loop closure  | measurement | measurement  |\n",
    "| Initial pose  | ignored     | prior factor |\n",
    "| Gauge freedom | âŒ unstable  | âœ… fixed      |\n",
    "| Drift         | unbounded   | controlled   |\n",
    "\n",
    "This is why **real SLAM is MAP**, not ML.\n",
    "\n",
    "---\n",
    "\n",
    "##  Intuition using force analogy\n",
    "\n",
    "Think of $x$ attached to springs:\n",
    "\n",
    "* Likelihood spring pulls toward measurement\n",
    "* Prior spring pulls toward belief\n",
    "\n",
    "| Case | Result          |\n",
    "| ---- | --------------- |\n",
    "| ML   | only one spring |\n",
    "| MAP  | two springs     |\n",
    "\n",
    "---\n",
    "\n",
    "##  When should you use which?\n",
    "\n",
    "| Situation                 | Use |\n",
    "| ------------------------- | --- |\n",
    "| Pure sensor calibration   | ML  |\n",
    "| Localization / SLAM       | MAP |\n",
    "| Filtering                 | MAP |\n",
    "| No prior knowledge at all | ML  |\n",
    "\n",
    "---\n",
    "\n",
    "##  One-line takeaway\n",
    "\n",
    "> **ML asks: \"Which $x$ best explains the data?\"\n",
    "> MAP asks: \"Which $x$ best explains the data *and* respects my prior belief?\"**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5deaa9-0d35-4dd0-ab02-86a0e16109f5",
   "metadata": {},
   "source": [
    "## Explaining what `gtsam.Marginals` \n",
    "\n",
    "Explaining what `gtsam.Marginals(graph, initial)` does mathematically:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Mathematical Explanation of `gtsam.Marginals(graph, initial)`\n",
    "\n",
    "When you call `gtsam.Marginals(graph, initial)`, GTSAM computes the marginal covariance matrices for each variable. Here's what happens:\n",
    "\n",
    "### 1. **Linearization at the Current Point**\n",
    "\n",
    "The factor graph defines a joint probability distribution:\n",
    "$$\n",
    "p(X_1, X_2, X_3) \\propto \\prod_i f_i(X_i)\n",
    "$$\n",
    "\n",
    "where each factor $f_i$ is a nonlinear function. GTSAM linearizes each factor around the current values in `initial` using a first-order Taylor expansion:\n",
    "\n",
    "$$\n",
    "f_i(X) \\approx f_i(\\bar{X}) + \\nabla f_i(\\bar{X})^T (X - \\bar{X})\n",
    "$$\n",
    "\n",
    "This converts the nonlinear factor graph into a Gaussian factor graph.\n",
    "\n",
    "### 2. **Forming the Information Matrix**\n",
    "\n",
    "The linearized factors are combined into a quadratic form. The joint distribution becomes approximately Gaussian:\n",
    "\n",
    "$$\n",
    "p(X_1, X_2, X_3) \\propto \\exp\\left(-\\frac{1}{2}(X - \\mu)^T \\Lambda (X - \\mu)\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu$ is the mean (from `initial`)\n",
    "- $\\Lambda$ is the information matrix (inverse covariance), built from the linearized factors\n",
    "\n",
    "### 3. **Building the Bayes Tree**\n",
    "\n",
    "GTSAM builds a Bayes Tree through variable elimination (e.g., Cholesky or QR). This exploits sparsity and enables efficient marginal computation.\n",
    "\n",
    "### 4. **Computing Marginal Covariances**\n",
    "\n",
    "To get the marginal covariance for variable \\(X_i\\), you need:\n",
    "\n",
    "$$\n",
    "\\Sigma_i = \\text{Cov}[X_i] = \\int \\cdots \\int p(X_1, X_2, X_3) \\, dX_j \\, dX_k \\quad \\text{(for } j,k \\neq i\\text{)}\n",
    "$$\n",
    "\n",
    "In matrix terms, if the full covariance is:\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} & \\Sigma_{13} \\\\ \\Sigma_{21} & \\Sigma_{22} & \\Sigma_{23} \\\\ \\Sigma_{31} & \\Sigma_{32} & \\Sigma_{33} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the marginal covariance for $X_1$ is just $\\Sigma_{11}$ (the diagonal block).\n",
    "\n",
    "### 5. **Efficient Computation via Bayes Tree**\n",
    "\n",
    "Instead of inverting the full information matrix $\\Lambda$ (expensive), GTSAM uses the Bayes Tree to compute marginals efficiently. When you call:\n",
    "```python\n",
    "marginals_before.marginalCovariance(X(1))\n",
    "```\n",
    "\n",
    "It extracts just the $\\Sigma_{11}$ block without computing the full $\\Sigma = \\Lambda^{-1}$.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. Linearization: The nonlinear graph is linearized around `initial`, so the marginals are approximate (valid near that point).\n",
    "2. Information form: GTSAM works in information form ($\\Lambda = \\Sigma^{-1}$) for efficiency.\n",
    "3. Sparse computation: The Bayes Tree structure allows computing marginals without full matrix inversion.\n",
    "4. Uncertainty quantification: The marginal covariance $\\Sigma_i$ quantifies uncertainty in variable $X_i$ given all constraints.\n",
    "\n",
    "### Why This Matters for Your Code:\n",
    "\n",
    "When you call `marginals_before.marginalCovariance(X(1))`, you get the 3Ã—3 covariance matrix for pose $X_1$ at the `initial` point, reflecting uncertainty from:\n",
    "- The prior on $X_1$\n",
    "- Odometry constraints\n",
    "- GPS constraints\n",
    "\n",
    "This covariance is what you visualize as the uncertainty ellipse. After optimization, the marginals are recomputed at the optimized solution, typically showing reduced uncertainty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
