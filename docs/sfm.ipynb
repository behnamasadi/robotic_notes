{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6487871c-11be-47af-85f3-a0ceb34bbb46",
   "metadata": {},
   "source": [
    "<img src=\"images/real.gif\" />\n",
    "\n",
    "<img src=\"images/virtual.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d681ec-532b-4ea5-9690-6c305e6d3bf6",
   "metadata": {},
   "source": [
    "## **Snavely Reprojection Error**\n",
    "\n",
    "### 1. **Camera Model Parameters**\n",
    "\n",
    "For `SnavelyReprojectionErrorFixedCamera`, we have:\n",
    "- **Camera parameters** (6D): $\\mathbf{c} = [\\omega_1, \\omega_2, \\omega_3, t_x, t_y, t_z]$\n",
    "  - $\\boldsymbol{\\omega} = [\\omega_1, \\omega_2, \\omega_3]^T$: angle-axis rotation (3D)\n",
    "  - $\\mathbf{t} = [t_x, t_y, t_z]^T$: translation (3D)\n",
    "- **3D Point**: $\\mathbf{P}_w = [X, Y, Z]^T$ in world coordinates\n",
    "- **Fixed intrinsics**: $f$ (focal length), $k_1, k_2$ (radial distortion coefficients)\n",
    "\n",
    "### 2. **Projection Pipeline**\n",
    "\n",
    "#### Step 1: Transform 3D point to camera coordinates\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_c = R(\\boldsymbol{\\omega}) \\mathbf{P}_w + \\mathbf{t}\n",
    "$$\n",
    "\n",
    "where $R(\\boldsymbol{\\omega})$ converts angle-axis to rotation matrix:\n",
    "\n",
    "```cpp\n",
    "ceres::AngleAxisRotatePoint(camera, point, p);\n",
    "p[0] += camera[3];  // tx\n",
    "p[1] += camera[4];  // ty  \n",
    "p[2] += camera[5];  // tz\n",
    "```\n",
    "\n",
    "So: $\\mathbf{P}_c = [p_x, p_y, p_z]^T$\n",
    "\n",
    "#### Step 2: Perspective projection (Snavely convention)\n",
    "\n",
    "Note the **negative sign** (Snavely's camera has negative z-axis):\n",
    "\n",
    "$$\n",
    "x_p = -\\frac{p_x}{p_z}, \\quad y_p = -\\frac{p_y}{p_z}\n",
    "$$\n",
    "\n",
    "```cpp\n",
    "const T xp = -p[0] / p[2];\n",
    "const T yp = -p[1] / p[2];\n",
    "```\n",
    "\n",
    "#### Step 3: Apply radial distortion\n",
    "\n",
    "$$\n",
    "r^2 = x_p^2 + y_p^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "d(r^2) = 1 + k_1 r^2 + k_2 r^4\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_d = d(r^2) \\cdot x_p, \\quad y_d = d(r^2) \\cdot y_p\n",
    "$$\n",
    "\n",
    "```cpp\n",
    "const T r2 = xp * xp + yp * yp;\n",
    "const T distortion = T(1.0) + r2 * (T(m_l1) + T(m_l2) * r2);\n",
    "```\n",
    "\n",
    "#### Step 4: Apply focal length to get pixel coordinates\n",
    "\n",
    "$$\n",
    "\\hat{x} = f \\cdot x_d, \\quad \\hat{y} = f \\cdot y_d\n",
    "$$\n",
    "\n",
    "```cpp\n",
    "const T predicted_x = T(m_focal) * distortion * xp;\n",
    "const T predicted_y = T(m_focal) * distortion * yp;\n",
    "```\n",
    "\n",
    "### 3. **Residual (Error) Computation**\n",
    "\n",
    "Given observed pixel coordinates $(x_{obs}, y_{obs})$:\n",
    "\n",
    "$$\n",
    "\\mathbf{r} = \\begin{bmatrix} r_x \\\\ r_y \\end{bmatrix} = \\begin{bmatrix} \\hat{x} - x_{obs} \\\\ \\hat{y} - y_{obs} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```cpp\n",
    "residuals[0] = predicted_x - T(m_observed_x);\n",
    "residuals[1] = predicted_y - T(m_observed_y);\n",
    "```\n",
    "\n",
    "### 4. **Ceres Optimization**\n",
    "\n",
    "Ceres minimizes the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{c}, \\{\\mathbf{P}_w^i\\}} \\sum_{i=1}^{N} \\|\\mathbf{r}_i(\\mathbf{c}, \\mathbf{P}_w^i)\\|^2 = \\sum_{i=1}^{N} (r_{x,i}^2 + r_{y,i}^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of 3D points\n",
    "- Each residual is 2D (x and y components)\n",
    "- Ceres uses automatic differentiation to compute Jacobians\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededbaf5-a511-45c2-82bb-a88ec9922026",
   "metadata": {},
   "source": [
    "## `SnavelyReprojectionErrorFixedCamera` \n",
    "\n",
    "### Key Difference from Standard `SnavelyReprojectionError`\n",
    "\n",
    "The `SnavelyReprojectionErrorFixedCamera` is a **specialized version** where certain camera parameters are **fixed** (not optimized):\n",
    "\n",
    "```cpp\n",
    "struct SnavelyReprojectionError {\n",
    "  // 9 parameters: 3 rotation + 3 translation + 1 focal + 2 distortion\n",
    "  // AutoDiffCostFunction<..., 2, 9, 3>  // 2D residual, 9D camera, 3D point\n",
    "};\n",
    "\n",
    "struct SnavelyReprojectionErrorFixedCamera {\n",
    "  // 6 parameters: 3 rotation + 3 translation (focal and distortion FIXED)\n",
    "  // AutoDiffCostFunction<..., 2, 6, 3>  // 2D residual, 6D camera, 3D point\n",
    "};\n",
    "```\n",
    "\n",
    "### What We're Doing in `SnavelyReprojectionErrorFixedCamera`\n",
    "\n",
    "#### **Constructor** (Lines 82-85)\n",
    "\n",
    "```82:85:src/src/snavely_reprojection_error.hpp\n",
    "SnavelyReprojectionErrorFixedCamera(double observed_x, double observed_y,\n",
    "                                    double fixed_focal, double fixed_l1, double fixed_l2, int block_id)\n",
    "    : m_observed_x(observed_x), m_observed_y(observed_y),\n",
    "      m_focal(fixed_focal), m_l1(fixed_l1), m_l2(fixed_l2),m_block_id(block_id) {}\n",
    "```\n",
    "\n",
    "We store:\n",
    "- $(x_{obs}, y_{obs})$: **observed** image coordinates (what we measured)\n",
    "- $f$: **fixed** focal length (known from calibration)\n",
    "- $k_1, k_2$: **fixed** distortion coefficients (known from calibration)\n",
    "- `block_id`: identifier for debugging\n",
    "\n",
    "#### **Optimization Variables** (Lines 88-90)\n",
    "\n",
    "```88:90:src/src/snavely_reprojection_error.hpp\n",
    "bool operator()(const T* const camera,\n",
    "                const T* const point,\n",
    "                T* residuals) const {\n",
    "```\n",
    "\n",
    "**Inputs to optimize:**\n",
    "- `camera[6]`: $[\\omega_1, \\omega_2, \\omega_3, t_x, t_y, t_z]$ - **variable** (being optimized)\n",
    "- `point[3]`: $[X, Y, Z]$ - **variable** (being optimized)\n",
    "\n",
    "**Fixed parameters** (not optimized):\n",
    "- `m_focal`: focal length $f$\n",
    "- `m_l1`, `m_l2`: distortion $k_1, k_2$\n",
    "\n",
    "#### **Why Use This Model?**\n",
    "\n",
    "This is useful when:\n",
    "\n",
    "1. ✅ **Camera intrinsics are known** from prior calibration\n",
    "2. ✅ **You want to optimize only extrinsics** (pose: rotation + translation) and 3D structure\n",
    "3. ✅ **Reduces degrees of freedom**: Fewer parameters = faster, more stable optimization\n",
    "4. ✅ **Prevents intrinsics drift**: Keeps focal length and distortion fixed\n",
    "\n",
    "### Complete Mathematical Flow in `SnavelyReprojectionErrorFixedCamera`\n",
    "\n",
    "Given:\n",
    "- Fixed: $f, k_1, k_2$ (intrinsics)\n",
    "- Optimize: $\\boldsymbol{\\omega}, \\mathbf{t}$ (camera pose), $\\mathbf{P}_w$ (3D points)\n",
    "\n",
    "**Forward projection:**\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_c = R(\\boldsymbol{\\omega}) \\mathbf{P}_w + \\mathbf{t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_p = -\\frac{P_{c,x}}{P_{c,z}}, \\quad y_p = -\\frac{P_{c,y}}{P_{c,z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "r^2 = x_p^2 + y_p^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "d = 1 + k_1 r^2 + k_2 r^4\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = f \\cdot d \\cdot x_p, \\quad \\hat{y} = f \\cdot d \\cdot y_p\n",
    "$$\n",
    "\n",
    "**Residual:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} r_x \\\\ r_y \\end{bmatrix} = \\begin{bmatrix} \\hat{x} - x_{obs} \\\\ \\hat{y} - y_{obs} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Ceres Optimization Problem\n",
    "\n",
    "```150:152:src/src/snavely_reprojection_error.hpp\n",
    "return (new ceres::AutoDiffCostFunction<SnavelyReprojectionErrorFixedCamera, 2, 6, 3>(\n",
    "            new SnavelyReprojectionErrorFixedCamera(observed_x, observed_y,\n",
    "                                                    fixed_focal, fixed_l1, fixed_l2,block_id)));\n",
    "```\n",
    "\n",
    "**Template parameters:**\n",
    "- `2`: residual dimension (x and y errors)\n",
    "- `6`: first parameter block dimension (camera: 3 rotation + 3 translation)\n",
    "- `3`: second parameter block dimension (point: X, Y, Z)\n",
    "\n",
    "**What Ceres does:**\n",
    "\n",
    "$$\n",
    "\\min_{\\substack{\\boldsymbol{\\omega}, \\mathbf{t} \\\\ \\{\\mathbf{P}_w^i\\}_{i=1}^N}} \\sum_{i=1}^{N} \\left\\| \\mathbf{r}_i(\\boldsymbol{\\omega}, \\mathbf{t}, \\mathbf{P}_w^i; f, k_1, k_2) \\right\\|^2\n",
    "$$\n",
    "\n",
    "- Optimizes: camera rotation $\\boldsymbol{\\omega}$, translation $\\mathbf{t}$, and all 3D points $\\mathbf{P}_w^i$\n",
    "- Keeps fixed: $f, k_1, k_2$\n",
    "- Uses **automatic differentiation** to compute Jacobians $\\frac{\\partial \\mathbf{r}}{\\partial \\boldsymbol{\\omega}}$, $\\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{t}}$, $\\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{P}_w}$\n",
    "- Solves using **sparse Levenberg-Marquardt** or other nonlinear least squares solver\n",
    "\n",
    "This is essentially **Pose + Structure optimization with known intrinsics** - a common scenario in visual SLAM and multi-view geometry!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0594e5d-cd44-4e6c-b870-33adf0603700",
   "metadata": {},
   "source": [
    "## Mathematical Formulation of `virtualCamIncrementalSfMFixedCam()`\n",
    "\n",
    "### 1. **Camera Pose Setup** (Lines 216-289)\n",
    "\n",
    "We have **3 cameras** with different poses in the world coordinate system:\n",
    "\n",
    "#### Camera 0 (Reference/World Camera):\n",
    "$$\n",
    "\\mathbf{R}_{C_0}^{W} = R_y(\\theta_{y,0}) \\quad \\text{where } \\theta_{y,0} = \\frac{\\pi}{12}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_{C_0}^{W} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```250:267:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "thetaCam0[0] = 0;\n",
    "thetaCam0[1] = +M_PI / 12;\n",
    "//  thetaCam0[1] = 0;\n",
    "thetaCam0[2] = 0;\n",
    "\n",
    "thetaCam1[0] = 0;\n",
    "thetaCam1[1] = +M_PI / 18;\n",
    "//  thetaCam1[1] = 0;\n",
    "thetaCam1[2] = 0;\n",
    "\n",
    "thetaCam2[0] = 0;\n",
    "thetaCam2[1] = -M_PI / 24;\n",
    "//  thetaCam2[1] = 0;\n",
    "thetaCam2[2] = 0;\n",
    "\n",
    "txCam0 = 0.0;\n",
    "tyCam0 = 0.0;\n",
    "tzCam0 = 0.0;\n",
    "```\n",
    "\n",
    "#### Camera 1:\n",
    "$$\n",
    "\\mathbf{R}_{C_1}^{W} = R_y(\\theta_{y,1}) \\quad \\text{where } \\theta_{y,1} = \\frac{\\pi}{18}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_{C_1}^{W} = \\begin{bmatrix} 0.9 \\\\ 0.1 \\\\ 0.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```269:272:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "//  txCam1 = 0.75;\n",
    "txCam1 = 0.9;\n",
    "tyCam1 = +0.1;\n",
    "tzCam1 = +0.3;\n",
    "```\n",
    "\n",
    "#### Camera 2:\n",
    "$$\n",
    "\\mathbf{R}_{C_2}^{W} = R_y(\\theta_{y,2}) \\quad \\text{where } \\theta_{y,2} = -\\frac{\\pi}{24}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_{C_2}^{W} = \\begin{bmatrix} 1.6 \\\\ -0.1 \\\\ 0.4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```274:276:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "txCam2 = +1.6;\n",
    "tyCam2 = -0.1;\n",
    "tzCam2 = +0.4;\n",
    "```\n",
    "\n",
    "**Note:** These are **Camera-in-World** poses (where the camera center is located in world coordinates).\n",
    "\n",
    "### 2. **3D Scene: Ellipsoid Points** (Lines 291-298)\n",
    "\n",
    "We create $N$ 3D points $\\{\\mathbf{P}_i^W\\}_{i=1}^{N}$ distributed on an ellipsoid surface:\n",
    "\n",
    "$$\n",
    "\\text{Ellipsoid center: } \\mathbf{c} = \\begin{bmatrix} -1.5 \\\\ 0 \\\\ -4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```294:298:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "float ellipsoidCenterX = -1.5;\n",
    "float ellipsoidCenterY = 0;\n",
    "float ellipsoidCenterZ = -4;\n",
    "objectPointsInWorldCoordinate = createEllipsoidInWorldCoordinate<cv::Vec3f>(\n",
    "    ellipsoidCenterX, ellipsoidCenterY, ellipsoidCenterZ);\n",
    "```\n",
    "\n",
    "Each point: $\\mathbf{P}_i^W = [X_i, Y_i, Z_i]^T$\n",
    "\n",
    "### 3. **Camera Intrinsic Parameters** (Lines 300-336)\n",
    "\n",
    "#### Distortion coefficients:\n",
    "$$\n",
    "k_1 = 0, \\quad k_2 = 0, \\quad p_1 = 0, \\quad p_2 = 0, \\quad k_3 = 0\n",
    "$$\n",
    "\n",
    "```305:309:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "k1 = 0.;\n",
    "k2 = 0.;\n",
    "p1 = 0.;\n",
    "p2 = 0.;\n",
    "k3 = 0.;\n",
    "```\n",
    "\n",
    "**(No distortion in this simulation)**\n",
    "\n",
    "#### Sensor and focal parameters:\n",
    "\n",
    "```314:334:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "unsigned int numberOfPixelInHeight, numberOfPixelInWidth;\n",
    "double heightOfSensor, widthOfSensor;\n",
    "double focalLength = 4.0;\n",
    "double mx, my, U0, V0;\n",
    "numberOfPixelInHeight = 600;\n",
    "numberOfPixelInWidth = 600;\n",
    "\n",
    "heightOfSensor = 10;\n",
    "widthOfSensor = 10;\n",
    "\n",
    "my = (numberOfPixelInHeight) / heightOfSensor;\n",
    "\n",
    "mx = (numberOfPixelInWidth) / widthOfSensor;\n",
    "\n",
    "double fx = focalLength * mx;\n",
    "double fy = focalLength * my;\n",
    "\n",
    "double cx, cy;\n",
    "\n",
    "cx = (numberOfPixelInWidth) / 2;\n",
    "cy = (numberOfPixelInHeight) / 2;\n",
    "```\n",
    "\n",
    "$$\n",
    "m_x = \\frac{\\text{pixels width}}{\\text{sensor width}} = \\frac{600}{10} = 60 \\text{ pixels/mm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_y = \\frac{\\text{pixels height}}{\\text{sensor height}} = \\frac{600}{10} = 60 \\text{ pixels/mm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_x = f \\cdot m_x = 4.0 \\times 60 = 240 \\text{ pixels}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_y = f \\cdot m_y = 4.0 \\times 60 = 240 \\text{ pixels}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_x = \\frac{600}{2} = 300 \\text{ pixels}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_y = \\frac{600}{2} = 300 \\text{ pixels}\n",
    "$$\n",
    "\n",
    "#### Camera intrinsic matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\begin{bmatrix} \n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "240 & 0 & 300 \\\\\n",
    "0 & 240 & 300 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 4. **Coordinate Transformation: World to Camera** (Lines 340-356)\n",
    "\n",
    "For each camera $j \\in \\{0, 1, 2\\}$, we need to transform world coordinates to camera coordinates.\n",
    "\n",
    "**Given:**\n",
    "- $\\mathbf{R}_{C_j}^{W}$: Rotation of camera $j$ in world frame\n",
    "- $\\mathbf{t}_{C_j}^{W}$: Translation of camera $j$ in world frame\n",
    "\n",
    "**We need:** Transformation that takes world points to camera frame.\n",
    "\n",
    "```340:341:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "cv::Mat rotation_world_in_Cam0 = rotation_Cam0_in_world.t();\n",
    "cv::Mat t_world_in_Cam0 = -rotation_Cam0_in_world.t() * t_Cam0_in_world;\n",
    "```\n",
    "\n",
    "**Transformation equations:**\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{W}^{C_j} = (\\mathbf{R}_{C_j}^{W})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_{W}^{C_j} = -(\\mathbf{R}_{C_j}^{W})^T \\mathbf{t}_{C_j}^{W}\n",
    "$$\n",
    "\n",
    "**Why?** Because:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}^{C_j} = \\mathbf{R}_{W}^{C_j} \\mathbf{P}^W + \\mathbf{t}_{W}^{C_j}\n",
    "$$\n",
    "\n",
    "This is derived from the inverse transformation:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}^W = \\mathbf{R}_{C_j}^{W} \\mathbf{P}^{C_j} + \\mathbf{t}_{C_j}^{W}\n",
    "$$\n",
    "\n",
    "Inverting:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}^{C_j} = (\\mathbf{R}_{C_j}^{W})^{-1} (\\mathbf{P}^W - \\mathbf{t}_{C_j}^{W}) = (\\mathbf{R}_{C_j}^{W})^T \\mathbf{P}^W - (\\mathbf{R}_{C_j}^{W})^T \\mathbf{t}_{C_j}^{W}\n",
    "$$\n",
    "\n",
    "### 5. **Projection to Image Plane** (Lines 343-356)\n",
    "\n",
    "Using OpenCV's `projectPoints`:\n",
    "\n",
    "```343:344:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "cv::projectPoints(objectPointsInWorldCoordinate, rotation_world_in_Cam0,\n",
    "                  t_world_in_Cam0, K, cv::noArray(), imagePointsCam0);\n",
    "```\n",
    "\n",
    "For each 3D point $\\mathbf{P}_i^W$ and camera $j$:\n",
    "\n",
    "**Step 1:** Transform to camera coordinates:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_i^{C_j} = \\mathbf{R}_{W}^{C_j} \\mathbf{P}_i^W + \\mathbf{t}_{W}^{C_j} = \\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2:** Project to normalized image plane:\n",
    "\n",
    "$$\n",
    "x_n = \\frac{X_c}{Z_c}, \\quad y_n = \\frac{Y_c}{Z_c}\n",
    "$$\n",
    "\n",
    "**Step 3:** Apply distortion (in this case, $k_1 = k_2 = 0$, so no distortion):\n",
    "\n",
    "$$\n",
    "x_d = x_n, \\quad y_d = y_n\n",
    "$$\n",
    "\n",
    "**Step 4:** Apply intrinsics to get pixel coordinates:\n",
    "\n",
    "$$\n",
    "u_{i,j} = f_x \\cdot x_d + c_x\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_{i,j} = f_y \\cdot y_d + c_y\n",
    "$$\n",
    "\n",
    "**Result:** For each camera $j$, we get image points:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{i,j} = [u_{i,j}, v_{i,j}]^T\n",
    "$$\n",
    "\n",
    "Stored in: `imagePointsCam0`, `imagePointsCam1`, `imagePointsCam2`\n",
    "\n",
    "### 6. **Point Correspondences** (Lines 468-504)\n",
    "\n",
    "We create **perfect correspondences** between all 3 cameras:\n",
    "\n",
    "```489:499:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "for (size_t i = 0; i < N_CAMERAS - 1; ++i) {\n",
    "\n",
    "  std::vector<cv::DMatch> matches;\n",
    "  // Generate matches based on point order, assumes all cameras have the same\n",
    "  // number of points\n",
    "  size_t num_points = keypoints[i].size();\n",
    "  for (size_t j = 0; j < num_points; ++j) {\n",
    "    // Match index j to j with a distance of 0.0\n",
    "    matches.emplace_back(cv::DMatch(j, j, 0.0f));\n",
    "    //      std::cout << \"j:\" << j << std::endl;\n",
    "  }\n",
    "```\n",
    "\n",
    "**Correspondence structure:**\n",
    "\n",
    "$$\n",
    "\\text{Point } i \\text{ in World} \\rightarrow \\begin{cases}\n",
    "\\mathbf{p}_{i,0} & \\text{in Camera 0} \\\\\n",
    "\\mathbf{p}_{i,1} & \\text{in Camera 1} \\\\\n",
    "\\mathbf{p}_{i,2} & \\text{in Camera 2}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "All points are visible in all cameras (complete track).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of What We Have\n",
    "\n",
    "**Known (Ground Truth):**\n",
    "- ✅ 3D points in world: $\\{\\mathbf{P}_i^W\\}_{i=1}^{N}$\n",
    "- ✅ Camera intrinsics: $\\mathbf{K}, k_1, k_2$ (no distortion)\n",
    "- ✅ Camera extrinsics: $\\mathbf{R}_{W}^{C_j}, \\mathbf{t}_{W}^{C_j}$ for $j \\in \\{0,1,2\\}$\n",
    "- ✅ Image projections: $\\{\\mathbf{p}_{i,j}\\}_{i=1}^{N}$ for each camera $j$\n",
    "- ✅ Perfect correspondences across all views\n",
    "\n",
    "**What SfM/Bundle Adjustment Will Estimate:**\n",
    "Given only:\n",
    "- Image points $\\{\\mathbf{p}_{i,j}\\}$\n",
    "- Intrinsics $\\mathbf{K}$ (fixed)\n",
    "\n",
    "Estimate:\n",
    "- Camera poses: $\\hat{\\mathbf{R}}_{W}^{C_j}, \\hat{\\mathbf{t}}_{W}^{C_j}$\n",
    "- 3D structure: $\\{\\hat{\\mathbf{P}}_i^W\\}_{i=1}^{N}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c8cbb-5939-4c32-aa3d-4c4e7769daed",
   "metadata": {},
   "source": [
    "### **Incremental SfM Pipeline (Lines 506-640)**\n",
    "\n",
    "#### 1. **Initialization** (Lines 507-521)\n",
    "\n",
    "Set Camera 0 as the **world reference frame**:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_0 = \\mathbf{I}_{3 \\times 3}, \\quad \\mathbf{t}_0 = \\mathbf{0}_{3 \\times 1}\n",
    "$$\n",
    "\n",
    "```508:510:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "std::vector<CameraExtrinsics> cameras(N_CAMERAS);\n",
    "cameras[0].R = cv::Mat::eye(3, 3, CV_64F);\n",
    "cameras[0].t = cv::Mat::zeros(3, 1, CV_64F);\n",
    "```\n",
    "\n",
    "#### 2. **Incremental Camera Addition Loop** (Lines 523-631)\n",
    "\n",
    "For each camera $i \\in \\{1, 2\\}$:\n",
    "\n",
    "##### **Step 2a: Essential Matrix & Relative Pose** (Lines 538-543)\n",
    "\n",
    "Find essential matrix between cameras $i-1$ and $i$:\n",
    "\n",
    "$$\n",
    "\\mathbf{E} = \\mathbf{K}^T [\\mathbf{t}_{i-1 \\to i}]_\\times \\mathbf{R}_{i-1 \\to i} \\mathbf{K}\n",
    "$$\n",
    "\n",
    "Recover relative pose:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{i-1 \\to i}, \\quad \\mathbf{t}_{i-1 \\to i}\n",
    "$$\n",
    "\n",
    "##### **Step 2b: Pose Composition** (Lines 545-554)\n",
    "\n",
    "Compose with previous camera to get global pose:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{0 \\to i} = \\mathbf{R}_{i-1 \\to i} \\cdot \\mathbf{R}_{0 \\to i-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_{0 \\to i} = \\mathbf{R}_{i-1 \\to i} \\cdot \\mathbf{t}_{0 \\to i-1} + \\mathbf{t}_{i-1 \\to i}\n",
    "$$\n",
    "\n",
    "**Verification:** Transform point from world (cam 0) to camera $i$:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}^{C_i} = \\mathbf{R}_{0 \\to i} \\mathbf{P}^{C_0} + \\mathbf{t}_{0 \\to i}\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "= \\mathbf{R}_{i-1 \\to i} (\\mathbf{R}_{0 \\to i-1} \\mathbf{P}^{C_0} + \\mathbf{t}_{0 \\to i-1}) + \\mathbf{t}_{i-1 \\to i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\mathbf{R}_{i-1 \\to i} \\mathbf{P}^{C_{i-1}} + \\mathbf{t}_{i-1 \\to i}\n",
    "$$\n",
    "\n",
    "✅ **Correct!**\n",
    "\n",
    "##### **Step 2c: Triangulation** (Lines 556-569)\n",
    "\n",
    "Build projection matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{i-1} = \\mathbf{K} [\\mathbf{R}_{0 \\to i-1} \\mid \\mathbf{t}_{0 \\to i-1}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{i} = \\mathbf{K} [\\mathbf{R}_{0 \\to i} \\mid \\mathbf{t}_{0 \\to i}]\n",
    "$$\n",
    "\n",
    "Triangulate 3D points using DLT (Direct Linear Transform).\n",
    "\n",
    "##### **Step 2d: Store Observations** (Lines 581-606)\n",
    "\n",
    "For each triangulated point $k$:\n",
    "\n",
    "```586:606:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "{\n",
    "  Observation obs;\n",
    "  obs.camera_idx = i - 1;\n",
    "  obs.point_idx = newPointIdx;\n",
    "\n",
    "  // negative sign for Snavely\n",
    "  obs.x = -(pts_im1[k].x - cx) / fx;\n",
    "  obs.y = -(pts_im1[k].y - cy) / fy;\n",
    "\n",
    "  observations.push_back(obs);\n",
    "}\n",
    "// Observation from camera (i):\n",
    "{\n",
    "  Observation obs;\n",
    "  obs.camera_idx = i;\n",
    "  obs.point_idx = newPointIdx;\n",
    "  // negative sign for Snavely\n",
    "  obs.x = -(pts_i[k].x - cx) / fx;\n",
    "  obs.y = -(pts_i[k].y - cy) / fy;\n",
    "  observations.push_back(obs);\n",
    "}\n",
    "```\n",
    "\n",
    "Convert to Snavely's normalized coordinates:\n",
    "\n",
    "$$\n",
    "x_{sn} = -\\frac{u - c_x}{f_x}, \\quad y_{sn} = -\\frac{v - c_y}{f_y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ **CRITICAL BUGS FOUND**\n",
    "\n",
    "### **Bug 1: Duplicate 3D Points Created** (Lines 581-606)\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Since you have **perfect correspondences** (all cameras see the same N points), the loop creates **duplicate 3D points**:\n",
    "\n",
    "- **Iteration 1** ($i=1$, cameras 0→1): Triangulate N points → indices 0 to N-1\n",
    "  - Camera 0 observes points 0...N-1\n",
    "  - Camera 1 observes points 0...N-1\n",
    "  \n",
    "- **Iteration 2** ($i=2$, cameras 1→2): Triangulate **the SAME N points again** → indices N to 2N-1\n",
    "  - Camera 1 observes points N...2N-1 **(DUPLICATE!)**\n",
    "  - Camera 2 observes points N...2N-1\n",
    "\n",
    "**Result:**\n",
    "- You have **2N 3D points** instead of N\n",
    "- Camera 1 has **duplicate observations** pointing to different 3D point indices for the same physical points\n",
    "- Bundle adjustment will fail or produce incorrect results\n",
    "\n",
    "**Example with N=100 points:**\n",
    "- `globalPoints3D.size() = 200` (should be 100)\n",
    "- Camera 1 has 200 observations (100 duplicates)\n",
    "- The optimization problem is ill-defined\n",
    "\n",
    "### **Bug 2: Incorrect Observation Structure**\n",
    "\n",
    "Camera 0 only has observations in iteration 1, but those observations don't cover all subsequent triangulated points. The structure is inconsistent.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Solution: Fix the Duplicate Point Issue**\n",
    "\n",
    "Since this is **synthetic data with perfect correspondences**, you should:\n",
    "\n",
    "1. **Triangulate points only ONCE** (in the first iteration)\n",
    "2. **Reuse existing 3D points** in subsequent iterations\n",
    "3. **Add observations for new cameras** viewing the same points\n",
    "\n",
    "Here's the corrected approach:\n",
    "\n",
    "```cpp\n",
    "// After line 521, add a map to track which match index corresponds to which 3D point\n",
    "std::map<int, int> matchIdx_to_pointIdx;\n",
    "\n",
    "for (size_t i = 1; i < N_CAMERAS; ++i) {\n",
    "  // ... existing code for essential matrix and pose recovery ...\n",
    "\n",
    "  std::vector<cv::Point2f> pts_im1, pts_i;\n",
    "  \n",
    "  for (const auto &match : all_matches[i - 1]) {\n",
    "    pts_im1.push_back(keypoints[i - 1][match.queryIdx].pt);\n",
    "    pts_i.push_back(keypoints[i][match.trainIdx].pt);\n",
    "  }\n",
    "\n",
    "  // ... essential matrix, pose recovery, triangulation ...\n",
    "\n",
    "  for (size_t k = 0; k < newPoints_in_cam0.size(); k++) {\n",
    "    int pointIdx;\n",
    "    \n",
    "    // FIRST ITERATION: Create new 3D points\n",
    "    if (i == 1) {\n",
    "      pointIdx = static_cast<int>(globalPoints3D.size());\n",
    "      globalPoints3D.push_back(newPoints_in_cam0[k]);\n",
    "      matchIdx_to_pointIdx[k] = pointIdx;  // Track correspondence\n",
    "      \n",
    "      // Add observation for camera 0\n",
    "      Observation obs0;\n",
    "      obs0.camera_idx = 0;\n",
    "      obs0.point_idx = pointIdx;\n",
    "      obs0.x = -(pts_im1[k].x - cx) / fx;\n",
    "      obs0.y = -(pts_im1[k].y - cy) / fy;\n",
    "      observations.push_back(obs0);\n",
    "    } \n",
    "    // SUBSEQUENT ITERATIONS: Reuse existing 3D points\n",
    "    else {\n",
    "      pointIdx = matchIdx_to_pointIdx[k];  // Use existing point index\n",
    "    }\n",
    "    \n",
    "    // Add observation for camera i (current camera)\n",
    "    Observation obs_i;\n",
    "    obs_i.camera_idx = i;\n",
    "    obs_i.point_idx = pointIdx;\n",
    "    obs_i.x = -(pts_i[k].x - cx) / fx;\n",
    "    obs_i.y = -(pts_i[k].y - cy) / fy;\n",
    "    observations.push_back(obs_i);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Expected Result After Fix:**\n",
    "\n",
    "With N=100 points and 3 cameras:\n",
    "- `globalPoints3D.size() = 100` ✅\n",
    "- Camera 0: 100 observations (points 0-99)\n",
    "- Camera 1: 100 observations (points 0-99)\n",
    "- Camera 2: 100 observations (points 0-99)\n",
    "- Total observations: 300 ✅\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f27ac1-1bcf-41c8-ab99-356982a0bbcf",
   "metadata": {},
   "source": [
    "## Coordinate Frame Analysis\n",
    "so when we do `recoverPose` we get the relative pose of `cam0` in `cam1` and pose of `cam1` in `cam2`, but we want all 3d points to be in `cam0`.\n",
    "\n",
    "\n",
    "## Coordinate Frame Analysis\n",
    "\n",
    "### What `recoverPose` Gives Us\n",
    "\n",
    "```542:543:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "cv::Mat R_im1_to_i, t_im1_to_i;\n",
    "cv::recoverPose(E, pts_im1, pts_i, K, R_im1_to_i, t_im1_to_i, mask);\n",
    "```\n",
    "\n",
    "`recoverPose` returns the **relative transformation**:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{i-1 \\to i}, \\quad \\mathbf{t}_{i-1 \\to i}\n",
    "$$\n",
    "\n",
    "This transforms points **FROM camera $i-1$ TO camera $i$**:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}^{C_i} = \\mathbf{R}_{i-1 \\to i} \\mathbf{P}^{C_{i-1}} + \\mathbf{t}_{i-1 \\to i}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "- **Iteration 1** ($i=1$): $\\mathbf{R}_{0 \\to 1}, \\mathbf{t}_{0 \\to 1}$ - transforms from cam0 to cam1\n",
    "- **Iteration 2** ($i=2$): $\\mathbf{R}_{1 \\to 2}, \\mathbf{t}_{1 \\to 2}$ - transforms from cam1 to cam2\n",
    "\n",
    "### Chaining Transformations to Cam0 Frame\n",
    "\n",
    "```545:552:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "cv::Mat R_0_to_im1 = cameras[i - 1].R;\n",
    "cv::Mat t_0_to_im1 = cameras[i - 1].t;\n",
    "\n",
    "cv::Mat R_0_to_i = R_im1_to_i * R_0_to_im1;\n",
    "cv::Mat t_0_to_i = R_im1_to_i * t_0_to_im1 + t_im1_to_i;\n",
    "\n",
    "cameras[i].R = R_0_to_i.clone();\n",
    "cameras[i].t = t_0_to_i.clone();\n",
    "```\n",
    "\n",
    "Since **Camera 0 is our world frame** ($\\mathbf{R}_0 = \\mathbf{I}, \\mathbf{t}_0 = \\mathbf{0}$), we chain the transformations:\n",
    "\n",
    "**For Camera 1:**\n",
    "$$\n",
    "\\mathbf{R}_{0 \\to 1} = \\mathbf{R}_{0 \\to 1} \\cdot \\mathbf{I} = \\mathbf{R}_{0 \\to 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_{0 \\to 1} = \\mathbf{R}_{0 \\to 1} \\cdot \\mathbf{0} + \\mathbf{t}_{0 \\to 1} = \\mathbf{t}_{0 \\to 1}\n",
    "$$\n",
    "\n",
    "**For Camera 2:**\n",
    "$$\n",
    "\\mathbf{R}_{0 \\to 2} = \\mathbf{R}_{1 \\to 2} \\cdot \\mathbf{R}_{0 \\to 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_{0 \\to 2} = \\mathbf{R}_{1 \\to 2} \\cdot \\mathbf{t}_{0 \\to 1} + \\mathbf{t}_{1 \\to 2}\n",
    "$$\n",
    "\n",
    "### Triangulation - Where Are the 3D Points?\n",
    "\n",
    "```556:569:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "cv::Mat Rt_im1(3, 4, CV_64F);\n",
    "cameras[i - 1].R.copyTo(Rt_im1(cv::Rect(0, 0, 3, 3)));\n",
    "cameras[i - 1].t.copyTo(Rt_im1(cv::Rect(3, 0, 1, 3)));\n",
    "cv::Mat P_im1 = K * Rt_im1;\n",
    "\n",
    "cv::Mat Rt_i(3, 4, CV_64F);\n",
    "cameras[i].R.copyTo(Rt_i(cv::Rect(0, 0, 3, 3)));\n",
    "cameras[i].t.copyTo(Rt_i(cv::Rect(3, 0, 1, 3)));\n",
    "cv::Mat P_i = K * Rt_i;\n",
    "\n",
    "cv::Mat points4D;\n",
    "\n",
    "cv::triangulatePoints(P_im1, P_i, pts_im1, pts_i, points4D);\n",
    "cv::Mat newPoints_in_cam0 = convertHomogeneous(points4D);\n",
    "```\n",
    "\n",
    "The projection matrices are:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{i-1} = \\mathbf{K} [\\mathbf{R}_{0 \\to i-1} \\mid \\mathbf{t}_{0 \\to i-1}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{i} = \\mathbf{K} [\\mathbf{R}_{0 \\to i} \\mid \\mathbf{t}_{0 \\to i}]\n",
    "$$\n",
    "\n",
    "These matrices **project 3D points FROM Cam0 frame TO image plane** of each camera:\n",
    "\n",
    "$$\n",
    "\\lambda \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\mathbf{P} \\begin{bmatrix} X_0 \\\\ Y_0 \\\\ Z_0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $[X_0, Y_0, Z_0]^T$ are coordinates in **Cam0/World frame**.\n",
    "\n",
    "**Therefore:** When `triangulatePoints` inverts this projection, it recovers points in **Cam0 frame**.\n",
    "\n",
    "### ✅ Answer: YES, All 3D Points Are in Cam0\n",
    "\n",
    "The variable name `newPoints_in_cam0` is **correct**! All triangulated points are expressed in Camera 0's coordinate system, which serves as the world reference frame.\n",
    "\n",
    "**Summary:**\n",
    "- Camera 0 frame = World frame (by choice)\n",
    "- All `cameras[i].R` and `cameras[i].t` represent: \"how to go from Cam0 to Camera i\"\n",
    "- All `globalPoints3D` are in Cam0/World coordinates\n",
    "- This is the standard setup for Bundle Adjustment ✅\n",
    "\n",
    "The coordinate frames are handled correctly in your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7dcd31-8d84-4ea8-bc01-928c42e4a98e",
   "metadata": {},
   "source": [
    "\n",
    "## Summary of Fixes\n",
    "\n",
    "### **Bug 1 - Duplicate 3D Points** ❌→✅\n",
    "\n",
    "**What was wrong:**\n",
    "```cpp\n",
    "// Every iteration created NEW points\n",
    "int newPointIdx = static_cast<int>(globalPoints3D.size());\n",
    "globalPoints3D.push_back(newPoints_in_cam0[k]);  // Always adds!\n",
    "```\n",
    "\n",
    "**Result with 100 points:**\n",
    "- Iteration 1: Creates points 0-99 (100 points)\n",
    "- Iteration 2: Creates points 100-199 (same 100 points duplicated!)\n",
    "- Total: 200 points instead of 100 ❌\n",
    "\n",
    "**Why it's wrong mathematically:**\n",
    "\n",
    "With perfect correspondences, match index \\(k\\) in **all** camera pairs refers to the **same physical 3D point**. Creating it multiple times violates this constraint:\n",
    "\n",
    "$$\n",
    "\\text{Match}_k: \\quad \\mathbf{P}_k^{\\text{world}} \\xrightarrow{\\text{project}} \\begin{cases} \n",
    "\\mathbf{p}_{k,0} & \\text{in cam0} \\\\\n",
    "\\mathbf{p}_{k,1} & \\text{in cam1} \\\\\n",
    "\\mathbf{p}_{k,2} & \\text{in cam2}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The buggy code created **two separate** \\(\\mathbf{P}_k\\) entries in `globalPoints3D`!\n",
    "\n",
    "### **Bug 2 - Missing/Duplicate Observations** ❌→✅\n",
    "\n",
    "**What was wrong:**\n",
    "```cpp\n",
    "// Added observation from camera i-1 in EVERY iteration\n",
    "obs.camera_idx = i - 1;  // Bug: cam1 gets observations in both i=1 and i=2!\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "- Camera 0: 100 observations (iteration 1 only) ✅\n",
    "- Camera 1: **200 observations** (100 in iteration 1, 100 more in iteration 2) ❌\n",
    "- Camera 2: 100 observations (iteration 2 only) ✅\n",
    "\n",
    "**Why it's wrong:**\n",
    "\n",
    "Camera 1 got duplicate observations pointing to **different** 3D point indices:\n",
    "- Iteration 1: cam1 observes points 0-99\n",
    "- Iteration 2: cam1 observes points 100-199 (same physical points!)\n",
    "\n",
    "Bundle adjustment sees this as camera 1 observing **200 different points** (wrong!).\n",
    "\n",
    "### **The Fix** ✅\n",
    "\n",
    "```cpp\n",
    "// Track correspondences\n",
    "std::map<int, int> matchIdx_to_pointIdx;\n",
    "\n",
    "if (i == 1) {\n",
    "  // FIRST iteration: Create points AND add cam0 observations\n",
    "  pointIdx = globalPoints3D.size();\n",
    "  globalPoints3D.push_back(newPoints_in_cam0[k]);\n",
    "  matchIdx_to_pointIdx[k] = pointIdx;\n",
    "  \n",
    "  // Add observation for cam0\n",
    "  obs_cam0.point_idx = pointIdx;\n",
    "  observations.push_back(obs_cam0);\n",
    "} else {\n",
    "  // SUBSEQUENT iterations: REUSE existing points\n",
    "  pointIdx = matchIdx_to_pointIdx[k];  // Use same index!\n",
    "}\n",
    "\n",
    "// Always add observation for NEW camera i\n",
    "obs_cam_i.point_idx = pointIdx;  // Same point index across all cameras\n",
    "observations.push_back(obs_cam_i);\n",
    "```\n",
    "\n",
    "**Correct result with N=100 points and 3 cameras:**\n",
    "- `globalPoints3D.size() = 100` ✅\n",
    "- Camera 0: 100 observations (points 0-99)\n",
    "- Camera 1: 100 observations (points 0-99, same indices!)\n",
    "- Camera 2: 100 observations (points 0-99, same indices!)\n",
    "- Total: 300 observations ✅\n",
    "\n",
    "This correctly represents the Bundle Adjustment structure where each physical point has ONE 3D coordinate and multiple 2D observations from different cameras!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad480f8-dcd3-4d66-8aba-d1af1891f0c5",
   "metadata": {},
   "source": [
    "\n",
    "## Detailed Explanation of Triangulation\n",
    "\n",
    "### **What Are Projection Matrices?**\n",
    "\n",
    "```562:570:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "cv::Mat Rt_im1(3, 4, CV_64F);\n",
    "cameras[i - 1].R.copyTo(Rt_im1(cv::Rect(0, 0, 3, 3)));\n",
    "cameras[i - 1].t.copyTo(Rt_im1(cv::Rect(3, 0, 1, 3)));\n",
    "cv::Mat P_im1 = K * Rt_im1;\n",
    "\n",
    "cv::Mat Rt_i(3, 4, CV_64F);\n",
    "cameras[i].R.copyTo(Rt_i(cv::Rect(0, 0, 3, 3)));\n",
    "cameras[i].t.copyTo(Rt_i(cv::Rect(3, 0, 1, 3)));\n",
    "cv::Mat P_i = K * Rt_i;\n",
    "```\n",
    "\n",
    "The projection matrices are constructed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{i-1} = \\mathbf{K} \\begin{bmatrix} \\mathbf{R}_{0 \\to i-1} & \\mathbf{t}_{0 \\to i-1} \\end{bmatrix}_{3 \\times 4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{i} = \\mathbf{K} \\begin{bmatrix} \\mathbf{R}_{0 \\to i} & \\mathbf{t}_{0 \\to i} \\end{bmatrix}_{3 \\times 4}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{R}_{0 \\to j}$: Rotation from **Cam0/World** to **Camera j**\n",
    "- $\\mathbf{t}_{0 \\to j}$: Translation from **Cam0/World** to **Camera j**\n",
    "\n",
    "### **Forward Projection (3D → 2D)**\n",
    "\n",
    "These matrices perform the **forward projection**: take a 3D point in **Cam0/World coordinates** and project it to image coordinates:\n",
    "\n",
    "$$\n",
    "\\lambda \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\mathbf{P}_j \\begin{bmatrix} X_0 \\\\ Y_0 \\\\ Z_0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Expanded form:**\n",
    "\n",
    "$$\n",
    "\\lambda \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\mathbf{K} \\begin{bmatrix} \\mathbf{R}_{0 \\to j} & \\mathbf{t}_{0 \\to j} \\end{bmatrix} \\begin{bmatrix} X_0 \\\\ Y_0 \\\\ Z_0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\mathbf{K} \\left( \\mathbf{R}_{0 \\to j} \\begin{bmatrix} X_0 \\\\ Y_0 \\\\ Z_0 \\end{bmatrix} + \\mathbf{t}_{0 \\to j} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\mathbf{K} \\begin{bmatrix} X_j \\\\ Y_j \\\\ Z_j \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $[X_j, Y_j, Z_j]^T$ are the point coordinates in **Camera j frame**.\n",
    "\n",
    "### **Triangulation (2D + 2D → 3D)**\n",
    "\n",
    "```574:575:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "cv::triangulatePoints(P_im1, P_i, pts_im1, pts_i, points4D);\n",
    "std::vector<cv::Point3f> newPoints_in_cam0 = convertHomogeneous(points4D);\n",
    "```\n",
    "\n",
    "**What `triangulatePoints` does:**\n",
    "\n",
    "Given:\n",
    "- $\\mathbf{P}_{i-1}$: Projection matrix for camera $i-1$\n",
    "- $\\mathbf{P}_{i}$: Projection matrix for camera $i$\n",
    "- $\\mathbf{p}_{i-1} = [u_{i-1}, v_{i-1}]^T$: Image point in camera $i-1$\n",
    "- $\\mathbf{p}_{i} = [u_{i}, v_{i}]^T$: Image point in camera $i$\n",
    "\n",
    "Find: $\\mathbf{P}_0 = [X_0, Y_0, Z_0, W]^T$ (homogeneous coordinates) such that:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{i-1} \\sim \\mathbf{P}_{i-1} \\mathbf{P}_0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{i} \\sim \\mathbf{P}_{i} \\mathbf{P}_0\n",
    "$$\n",
    "\n",
    "where $\\sim$ means \"equal up to scale\" (projective equality).\n",
    "\n",
    "### **Mathematical Solution: DLT (Direct Linear Transform)**\n",
    "\n",
    "For each image point, we have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} \\times \\left( \\mathbf{P} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ W \\end{bmatrix} \\right) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Cross product gives 2 independent equations (the third is redundant):\n",
    "\n",
    "$$\n",
    "u \\cdot \\mathbf{p}_3^T \\mathbf{X} - \\mathbf{p}_1^T \\mathbf{X} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "v \\cdot \\mathbf{p}_3^T \\mathbf{X} - \\mathbf{p}_2^T \\mathbf{X} = 0\n",
    "$$\n",
    "\n",
    "where $\\mathbf{p}_j^T$ is the $j$-th row of $\\mathbf{P}$, and $\\mathbf{X} = [X, Y, Z, W]^T$.\n",
    "\n",
    "**From two views**, we get 4 equations (2 per view):\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{X} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}$ is a $4 \\times 4$ matrix. Solve using SVD to find $\\mathbf{X}$.\n",
    "\n",
    "### **Coordinate Frame of the Result**\n",
    "\n",
    "**Key insight:** The projection matrices $\\mathbf{P}_{i-1}$ and $\\mathbf{P}_i$ encode transformations **FROM Cam0** coordinates. When triangulation inverts these projections, it recovers points in **Cam0 coordinates**.\n",
    "\n",
    "**Why?** Because the system being solved is:\n",
    "\n",
    "$$\n",
    "\\lambda_{i-1} \\mathbf{p}_{i-1} = \\mathbf{K} \\left( \\mathbf{R}_{0 \\to i-1} \\mathbf{P}_0 + \\mathbf{t}_{0 \\to i-1} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_{i} \\mathbf{p}_{i} = \\mathbf{K} \\left( \\mathbf{R}_{0 \\to i} \\mathbf{P}_0 + \\mathbf{t}_{0 \\to i} \\right)\n",
    "$$\n",
    "\n",
    "The unknown is $\\mathbf{P}_0 = [X_0, Y_0, Z_0]^T$, which are coordinates in **Cam0 frame** (world frame).\n",
    "\n",
    "### **Verification**\n",
    "\n",
    "The variable name confirms this:\n",
    "\n",
    "```575:575:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "std::vector<cv::Point3f> newPoints_in_cam0 = convertHomogeneous(points4D);\n",
    "```\n",
    "\n",
    "`newPoints_in_cam0` are in **Cam0 coordinates**, which is the **world reference frame** in your setup.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Component | What It Does | Coordinate Frame |\n",
    "|-----------|-------------|------------------|\n",
    "| $\\mathbf{P}_{i-1}, \\mathbf{P}_i$ | Project **FROM Cam0/World TO image** | Input: Cam0, Output: Image |\n",
    "| `triangulatePoints` | **Inverse**: recover 3D from images | Output: **Cam0/World** |\n",
    "| `points4D` | Result in homogeneous coords $[X_0, Y_0, Z_0, W]^T$ | **Cam0/World** |\n",
    "| `newPoints_in_cam0` | Result in Cartesian coords $[X_0/W, Y_0/W, Z_0/W]^T$ | **Cam0/World** ✅ |\n",
    "\n",
    "**Answer:** YES, `points4D` and `newPoints_in_cam0` are in **Cam0/World coordinates**! ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffccb0-cb21-43d7-9b83-180709606a3c",
   "metadata": {},
   "source": [
    "\n",
    "## Simple Example Setup\n",
    "\n",
    "Assume we have:\n",
    "- **3 cameras**: Cam0, Cam1, Cam2\n",
    "- **3 physical 3D points**: Point A, Point B, Point C\n",
    "- All points visible in all cameras (perfect correspondences)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: `cv::DMatch` and `all_matches` (Lines 469-505)\n",
    "\n",
    "### **What is `cv::DMatch`?**\n",
    "\n",
    "```cpp\n",
    "cv::DMatch(queryIdx, trainIdx, distance)\n",
    "```\n",
    "\n",
    "- `queryIdx`: Index of keypoint in the **first/query** image\n",
    "- `trainIdx`: Index of keypoint in the **second/train** image  \n",
    "- `distance`: Match quality (0.0 = perfect match)\n",
    "\n",
    "### **Building Matches** (Lines 489-505)\n",
    "\n",
    "```489:505:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "for (size_t i = 0; i < N_CAMERAS - 1; ++i) {\n",
    "\n",
    "  std::vector<cv::DMatch> matches;\n",
    "  // Generate matches based on point order, assumes all cameras have the same\n",
    "  // number of points\n",
    "  size_t num_points = keypoints[i].size();\n",
    "  for (size_t j = 0; j < num_points; ++j) {\n",
    "    // Match index j to j with a distance of 0.0\n",
    "    matches.emplace_back(cv::DMatch(j, j, 0.0f));\n",
    "    //      std::cout << \"j:\" << j << std::endl;\n",
    "  }\n",
    "\n",
    "  drawMatchesBetweenTheTwoFrames(images[i], images[i + 1], keypoints[i],\n",
    "                                 keypoints[i + 1], matches);\n",
    "\n",
    "  all_matches.push_back(matches);\n",
    "}\n",
    "```\n",
    "\n",
    "### **Numerical Example:**\n",
    "\n",
    "#### Keypoints in each camera:\n",
    "\n",
    "| Physical Point | Cam0 Index | Pixel (u,v) | Cam1 Index | Pixel (u,v) | Cam2 Index | Pixel (u,v) |\n",
    "|----------------|------------|-------------|------------|-------------|------------|-------------|\n",
    "| Point A | 0 | (250, 280) | 0 | (260, 285) | 0 | (270, 290) |\n",
    "| Point B | 1 | (320, 310) | 1 | (330, 315) | 1 | (340, 320) |\n",
    "| Point C | 2 | (380, 340) | 2 | (390, 345) | 2 | (400, 350) |\n",
    "\n",
    "#### Building `all_matches`:\n",
    "\n",
    "**Iteration i=0 (Cam0 ↔ Cam1):**\n",
    "```cpp\n",
    "matches[0] = cv::DMatch(0, 0, 0.0)  // Cam0[0] ↔ Cam1[0] (Point A)\n",
    "matches[1] = cv::DMatch(1, 1, 0.0)  // Cam0[1] ↔ Cam1[1] (Point B)\n",
    "matches[2] = cv::DMatch(2, 2, 0.0)  // Cam0[2] ↔ Cam1[2] (Point C)\n",
    "all_matches[0] = matches  // Store matches between cam0 and cam1\n",
    "```\n",
    "\n",
    "**Iteration i=1 (Cam1 ↔ Cam2):**\n",
    "```cpp\n",
    "matches[0] = cv::DMatch(0, 0, 0.0)  // Cam1[0] ↔ Cam2[0] (Point A)\n",
    "matches[1] = cv::DMatch(1, 1, 0.0)  // Cam1[1] ↔ Cam2[1] (Point B)\n",
    "matches[2] = cv::DMatch(2, 2, 0.0)  // Cam1[2] ↔ Cam2[2] (Point C)\n",
    "all_matches[1] = matches  // Store matches between cam1 and cam2\n",
    "```\n",
    "\n",
    "**Final `all_matches` structure:**\n",
    "```\n",
    "all_matches[0]: Matches between Cam0 and Cam1 (3 matches)\n",
    "all_matches[1]: Matches between Cam1 and Cam2 (3 matches)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: `Observation` Structure (Lines 208-212)\n",
    "\n",
    "```208:212:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "struct Observation {\n",
    "  int camera_idx; // which camera sees this point\n",
    "  int point_idx;  // which 3D point is observed\n",
    "  double x, y;    // 2D feature coords\n",
    "};\n",
    "```\n",
    "\n",
    "### **What is an Observation?**\n",
    "\n",
    "An observation links:\n",
    "- A **camera** (by index)\n",
    "- A **3D point** (by index in `globalPoints3D`)\n",
    "- The **2D projection** of that point in that camera (in Snavely normalized coords)\n",
    "\n",
    "### **Building Observations in Incremental SfM Loop**\n",
    "\n",
    "Let's trace through with our 3-point example:\n",
    "\n",
    "#### **Iteration i=1 (Adding Cam1, triangulating between Cam0 and Cam1):**\n",
    "\n",
    "From the corrected code:\n",
    "\n",
    "```cpp\n",
    "for (size_t k = 0; k < newPoints_in_cam0.size(); k++) {  // k = 0, 1, 2\n",
    "  \n",
    "  if (i == 1) {  // FIRST ITERATION\n",
    "    // Create 3D point\n",
    "    pointIdx = globalPoints3D.size();\n",
    "    globalPoints3D.push_back(newPoints_in_cam0[k]);\n",
    "    matchIdx_to_pointIdx[k] = pointIdx;\n",
    "    \n",
    "    // Add observation from Cam0\n",
    "    Observation obs_cam0;\n",
    "    obs_cam0.camera_idx = 0;\n",
    "    obs_cam0.point_idx = pointIdx;\n",
    "    obs_cam0.x = -(pts_im1[k].x - cx) / fx;\n",
    "    obs_cam0.y = -(pts_im1[k].y - cy) / fy;\n",
    "    observations.push_back(obs_cam0);\n",
    "  }\n",
    "  \n",
    "  // Add observation from Cam1\n",
    "  Observation obs_cam_i;\n",
    "  obs_cam_i.camera_idx = i;  // i=1\n",
    "  obs_cam_i.point_idx = pointIdx;\n",
    "  obs_cam_i.x = -(pts_i[k].x - cx) / fx;\n",
    "  obs_cam_i.y = -(pts_i[k].y - cy) / fy;\n",
    "  observations.push_back(obs_cam_i);\n",
    "}\n",
    "```\n",
    "\n",
    "**Numerical example (k=0, Point A):**\n",
    "\n",
    "Assume after normalization:\n",
    "- Cam0 sees Point A at normalized coords: `x=-0.042, y=-0.083`\n",
    "- Cam1 sees Point A at normalized coords: `x=-0.033, y=-0.073`\n",
    "\n",
    "**Created observations:**\n",
    "```cpp\n",
    "observations[0] = {camera_idx: 0, point_idx: 0, x: -0.042, y: -0.083}  // Cam0 sees Point A\n",
    "observations[1] = {camera_idx: 1, point_idx: 0, x: -0.033, y: -0.073}  // Cam1 sees Point A\n",
    "```\n",
    "\n",
    "**After k=0,1,2 in iteration i=1:**\n",
    "\n",
    "| Index | camera_idx | point_idx | x | y | Meaning |\n",
    "|-------|------------|-----------|---|---|---------|\n",
    "| 0 | 0 | 0 | -0.042 | -0.083 | Cam0 sees Point A |\n",
    "| 1 | 1 | 0 | -0.033 | -0.073 | Cam1 sees Point A |\n",
    "| 2 | 0 | 1 | -0.033 | -0.042 | Cam0 sees Point B |\n",
    "| 3 | 1 | 1 | -0.025 | -0.031 | Cam1 sees Point B |\n",
    "| 4 | 0 | 2 | 0.033 | 0.000 | Cam0 sees Point C |\n",
    "| 5 | 1 | 2 | 0.042 | 0.010 | Cam1 sees Point C |\n",
    "\n",
    "**State after iteration i=1:**\n",
    "- `globalPoints3D.size() = 3` (Points A, B, C)\n",
    "- `observations.size() = 6` (2 cameras × 3 points)\n",
    "\n",
    "#### **Iteration i=2 (Adding Cam2, triangulating between Cam1 and Cam2):**\n",
    "\n",
    "```cpp\n",
    "for (size_t k = 0; k < newPoints_in_cam0.size(); k++) {  // k = 0, 1, 2\n",
    "  \n",
    "  if (i == 1) {\n",
    "    // SKIP (only for first iteration)\n",
    "  } else {\n",
    "    // REUSE existing point\n",
    "    pointIdx = matchIdx_to_pointIdx[k];  // Get existing index\n",
    "  }\n",
    "  \n",
    "  // Add observation from Cam2\n",
    "  Observation obs_cam_i;\n",
    "  obs_cam_i.camera_idx = i;  // i=2\n",
    "  obs_cam_i.point_idx = pointIdx;  // REUSE same point index!\n",
    "  obs_cam_i.x = -(pts_i[k].x - cx) / fx;\n",
    "  obs_cam_i.y = -(pts_i[k].y - cy) / fy;\n",
    "  observations.push_back(obs_cam_i);\n",
    "}\n",
    "```\n",
    "\n",
    "**After k=0,1,2 in iteration i=2 (adding 3 more observations):**\n",
    "\n",
    "| Index | camera_idx | point_idx | x | y | Meaning |\n",
    "|-------|------------|-----------|---|---|---------|\n",
    "| 0-5 | ... | ... | ... | ... | (Previous 6 observations) |\n",
    "| 6 | 2 | 0 | -0.025 | -0.063 | Cam2 sees Point A |\n",
    "| 7 | 2 | 1 | -0.017 | -0.021 | Cam2 sees Point B |\n",
    "| 8 | 2 | 2 | 0.050 | 0.020 | Cam2 sees Point C |\n",
    "\n",
    "**Final state after all iterations:**\n",
    "- `globalPoints3D.size() = 3` ✅ (Points A, B, C)\n",
    "- `observations.size() = 9` ✅ (3 cameras × 3 points)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Using Observations in Bundle Adjustment (Lines 729-743)\n",
    "\n",
    "```728:743:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "int id = 0;\n",
    "for (const Observation &obs : observations) {\n",
    "\n",
    "  ceres::CostFunction *costFunc = SnavelyReprojectionErrorFixedCamera::Create(\n",
    "      obs.x, obs.y, 1, k1, k2, id);\n",
    "  id++;\n",
    "\n",
    "  double *cameraParamsPtr = &cameraParams[obs.camera_idx * 6];\n",
    "  double *pointPtr = &pointParams[3 * obs.point_idx];\n",
    "  // Create the reprojection error cost:\n",
    "\n",
    "  // Add a residual block:\n",
    "  problem.AddResidualBlock(costFunc,\n",
    "                           nullptr, // squared loss\n",
    "                           cameraParamsPtr, pointPtr);\n",
    "}\n",
    "```\n",
    "\n",
    "### **What This Does:**\n",
    "\n",
    "For each observation, create a residual block that computes:\n",
    "\n",
    "$$\n",
    "\\text{residual} = \\text{predicted}(camera_i, point_j) - \\text{observed}(x, y)\n",
    "$$\n",
    "\n",
    "### **Numerical Example - Observation 0:**\n",
    "\n",
    "```cpp\n",
    "obs = observations[0] = {camera_idx: 0, point_idx: 0, x: -0.042, y: -0.083}\n",
    "```\n",
    "\n",
    "**Creates residual:**\n",
    "- `cameraParamsPtr = &cameraParams[0 * 6]` → Points to camera 0's parameters (6 values: rotation + translation)\n",
    "- `pointPtr = &pointParams[3 * 0]` → Points to Point A's 3D coordinates (3 values: X, Y, Z)\n",
    "- Cost function: Compare projection of `pointParams[0,1,2]` through `cameraParams[0-5]` with observed `(-0.042, -0.083)`\n",
    "\n",
    "### **Visual Summary:**\n",
    "\n",
    "```\n",
    "Ceres Optimization Problem Structure:\n",
    "======================================\n",
    "\n",
    "Camera Parameters:\n",
    "  cameraParams[0-5]:   Cam0 (ω₁, ω₂, ω₃, tₓ, tᵧ, tᵤ)\n",
    "  cameraParams[6-11]:  Cam1 (ω₁, ω₂, ω₃, tₓ, tᵧ, tᵤ)\n",
    "  cameraParams[12-17]: Cam2 (ω₁, ω₂, ω₃, tₓ, tᵧ, tᵤ)\n",
    "\n",
    "3D Points:\n",
    "  pointParams[0-2]:    Point A (X, Y, Z)\n",
    "  pointParams[3-5]:    Point B (X, Y, Z)\n",
    "  pointParams[6-8]:    Point C (X, Y, Z)\n",
    "\n",
    "Residual Blocks (9 total):\n",
    "  [0] Cam0 → Point A: observed (-0.042, -0.083)\n",
    "  [1] Cam1 → Point A: observed (-0.033, -0.073)\n",
    "  [2] Cam0 → Point B: observed (-0.033, -0.042)\n",
    "  [3] Cam1 → Point B: observed (-0.025, -0.031)\n",
    "  [4] Cam0 → Point C: observed (0.033, 0.000)\n",
    "  [5] Cam1 → Point C: observed (0.042, 0.010)\n",
    "  [6] Cam2 → Point A: observed (-0.025, -0.063)\n",
    "  [7] Cam2 → Point B: observed (-0.017, -0.021)\n",
    "  [8] Cam2 → Point C: observed (0.050, 0.020)\n",
    "```\n",
    "\n",
    "Each residual block creates a **2D error** (x and y), so total: **9 residual blocks × 2 dimensions = 18 equations** to optimize **24 parameters** (18 camera params + 9 point coords - but Cam0 is fixed, so actually 21 parameters).\n",
    "\n",
    "This is the classic **Bundle Adjustment** sparse optimization problem! ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2a699-ec08-482a-a201-3c48dd25d35e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Explanation of the `Observation` Struct\n",
    "\n",
    "The `Observation` struct is a fundamental data structure in Structure from Motion (SfM) that links 2D image measurements to 3D world points. \n",
    "\n",
    "```244:248:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "struct Observation {\n",
    "  int camera_idx; // which camera sees this point\n",
    "  int point_idx;  // which 3D point is observed\n",
    "  double x, y;    // 2D feature coords (Snavely normalized)\n",
    "};\n",
    "```\n",
    "\n",
    "### Components:\n",
    "\n",
    "1. **`camera_idx`**: Index identifying which camera observed the point (0, 1, 2, ...)\n",
    "2. **`point_idx`**: Index of the corresponding 3D point in the global point cloud\n",
    "3. **`x, y`**: 2D feature coordinates in **Snavely normalized** coordinates (not pixel coordinates!)\n",
    "\n",
    "### Snavely Normalized Coordinates\n",
    "\n",
    "The key detail is that `(x, y)` are **NOT** pixel coordinates. They are normalized according to the Snavely/Bundler camera model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{\\text{Snavely}} &= -\\frac{u - c_x}{f_x} \\\\\n",
    "y_{\\text{Snavely}} &= -\\frac{v - c_y}{f_y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $(u, v)$ = pixel coordinates\n",
    "- $(c_x, c_y)$ = principal point (image center)\n",
    "- $f_x, f_y$ = focal lengths\n",
    "- The **negative sign** is a Bundler convention (camera looks down negative Z-axis)\n",
    "\n",
    "You can see this conversion happening in the code:\n",
    "\n",
    "```1282:1285:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "      // Convert from pixel coordinates to Snavely normalized coordinates\n",
    "      // Snavely convention: negative sign and normalize by focal length\n",
    "      obs.x = -(feat_obs.pixel.x - cx) / fx;\n",
    "      obs.y = -(feat_obs.pixel.y - cy) / fy;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Numerical Examples\n",
    "\n",
    "Let me provide concrete examples to illustrate how `Observation` works:\n",
    "\n",
    "### Example 1: Simple Single Observation\n",
    "\n",
    "**Scenario**: Camera 0 observes 3D point #5 at pixel location (320, 240)\n",
    "\n",
    "**Given**:\n",
    "- Camera intrinsics: $f_x = 800$, $f_y = 800$, $c_x = 320$, $c_y = 240$\n",
    "- Pixel coordinates: $(u, v) = (320, 240)$\n",
    "\n",
    "**Compute Snavely coordinates**:\n",
    "```\n",
    "x_Snavely = -(320 - 320) / 800 = 0.0\n",
    "y_Snavely = -(240 - 240) / 800 = 0.0\n",
    "```\n",
    "\n",
    "**Resulting Observation**:\n",
    "```cpp\n",
    "Observation obs;\n",
    "obs.camera_idx = 0;     // Camera 0\n",
    "obs.point_idx = 5;      // 3D point #5\n",
    "obs.x = 0.0;            // Center of image\n",
    "obs.y = 0.0;            // Center of image\n",
    "```\n",
    "\n",
    "### Example 2: Off-Center Feature\n",
    "\n",
    "**Scenario**: Camera 1 observes 3D point #12 at pixel (720, 340)\n",
    "\n",
    "**Given**:\n",
    "- Camera intrinsics: $f_x = 800$, $f_y = 800$, $c_x = 320$, $c_y = 240$\n",
    "- Pixel coordinates: $(u, v) = (720, 340)$\n",
    "\n",
    "**Compute Snavely coordinates**:\n",
    "```\n",
    "x_Snavely = -(720 - 320) / 800 = -400 / 800 = -0.5\n",
    "y_Snavely = -(340 - 240) / 800 = -100 / 800 = -0.125\n",
    "```\n",
    "\n",
    "**Resulting Observation**:\n",
    "```cpp\n",
    "Observation obs;\n",
    "obs.camera_idx = 1;     // Camera 1\n",
    "obs.point_idx = 12;     // 3D point #12\n",
    "obs.x = -0.5;           // Right of center (negative due to Snavely convention)\n",
    "obs.y = -0.125;         // Below center\n",
    "```\n",
    "\n",
    "### Example 3: Multi-View Bundle Adjustment\n",
    "\n",
    "**Scenario**: The same 3D point is seen by three cameras\n",
    "\n",
    "Suppose 3D point #8 (a corner of a building) is visible in all three cameras:\n",
    "\n",
    "| Camera | Pixel (u, v) | Snavely (x, y) | Observation |\n",
    "|--------|--------------|----------------|-------------|\n",
    "| 0      | (450, 300)   | (-0.1625, -0.075) | `{0, 8, -0.1625, -0.075}` |\n",
    "| 1      | (280, 220)   | (0.05, 0.025)  | `{1, 8, 0.05, 0.025}` |\n",
    "| 2      | (520, 380)   | (-0.25, -0.175) | `{2, 8, -0.25, -0.175}` |\n",
    "\n",
    "**Calculations** (assuming $f = 800$, $c_x = 320$, $c_y = 240$):\n",
    "\n",
    "**Camera 0**:\n",
    "```\n",
    "x = -(450 - 320) / 800 = -130 / 800 = -0.1625\n",
    "y = -(300 - 240) / 800 = -60 / 800 = -0.075\n",
    "```\n",
    "\n",
    "**Camera 1**:\n",
    "```\n",
    "x = -(280 - 320) / 800 = 40 / 800 = 0.05\n",
    "y = -(220 - 240) / 800 = 20 / 800 = 0.025\n",
    "```\n",
    "\n",
    "**Camera 2**:\n",
    "```\n",
    "x = -(520 - 320) / 800 = -200 / 800 = -0.25\n",
    "y = -(380 - 240) / 800 = -140 / 800 = -0.175\n",
    "```\n",
    "\n",
    "This creates **3 observations** for the same 3D point, which is critical for bundle adjustment to triangulate and refine the 3D location.\n",
    "\n",
    "### Example 4: Complete Mini-System\n",
    "\n",
    "**System Setup**:\n",
    "- 2 cameras\n",
    "- 3 3D points\n",
    "- Total: 5 observations (some points not visible in all cameras)\n",
    "\n",
    "```cpp\n",
    "std::vector<Observation> observations = {\n",
    "    // Point 0 seen by both cameras\n",
    "    {0, 0, -0.1,  0.05},   // Camera 0 sees point 0\n",
    "    {1, 0,  0.12, -0.08},  // Camera 1 sees point 0\n",
    "    \n",
    "    // Point 1 seen only by camera 0\n",
    "    {0, 1, -0.3,  0.2},    // Camera 0 sees point 1\n",
    "    \n",
    "    // Point 2 seen by both cameras\n",
    "    {0, 2,  0.15, -0.1},   // Camera 0 sees point 2\n",
    "    {1, 2, -0.2,   0.15}   // Camera 1 sees point 2\n",
    "};\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Point 0: Visible in 2 views → strong triangulation\n",
    "- Point 1: Visible in 1 view → cannot be refined (needs ≥2 views)\n",
    "- Point 2: Visible in 2 views → strong triangulation\n",
    "\n",
    "---\n",
    "\n",
    "## How Observations Are Used in Bundle Adjustment\n",
    "\n",
    "Each `Observation` creates a **residual block** in Ceres optimization:\n",
    "\n",
    "```1328:1339:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "  for (const Observation &obs : observations) {\n",
    "\n",
    "    ceres::CostFunction *costFunc = SnavelyReprojectionErrorFixedCamera::Create(\n",
    "        obs.x, obs.y, 1, k1, k2, id);\n",
    "    id++;\n",
    "\n",
    "    double *cameraParamsPtr = &cameraParams[obs.camera_idx * 6];\n",
    "    double *pointPtr = &pointParams[3 * obs.point_idx];\n",
    "    // Create the reprojection error cost:\n",
    "\n",
    "    // Add a residual block:\n",
    "    problem.AddResidualBlock(costFunc,\n",
    "```\n",
    "\n",
    "The optimization minimizes the **reprojection error**:\n",
    "\n",
    "$$\n",
    "\\text{error} = \\left\\| \\text{predicted}_{x,y} - \\text{observed}_{x,y} \\right\\|^2\n",
    "$$\n",
    "\n",
    "where `observed` comes from `obs.x, obs.y` and `predicted` is computed by projecting the 3D point through the camera model.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The `Observation` struct elegantly encodes the relationship:\n",
    "> \"Camera `i` observed 3D point `j` at normalized coordinates `(x, y)`\"\n",
    "\n",
    "This simple structure enables powerful multi-view geometry optimization through bundle adjustment, where all camera poses and 3D points are refined simultaneously to minimize reprojection errors across all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7bee73-992b-44ef-bccc-ca337541d9d8",
   "metadata": {},
   "source": [
    "\n",
    "## Overview: Track-Based Structure from Motion\n",
    "\n",
    "These structures implement the **track** concept - a fundamental building block in modern SfM pipelines. A track represents a single physical 3D point that has been observed across multiple camera views.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. FeatureObservation Struct\n",
    "\n",
    "```266:273:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "struct FeatureObservation {\n",
    "  int camera_idx;    // Which camera sees this feature\n",
    "  int feature_idx;   // Index of the keypoint in that camera's keypoint list\n",
    "  cv::Point2f pixel; // Original pixel coordinates (u, v)\n",
    "\n",
    "  FeatureObservation(int cam, int feat, const cv::Point2f &px)\n",
    "      : camera_idx(cam), feature_idx(feat), pixel(px) {}\n",
    "};\n",
    "```\n",
    "\n",
    "### Components:\n",
    "\n",
    "- **`camera_idx`**: Which camera captured this observation (e.g., 0, 1, 2, ...)\n",
    "- **`feature_idx`**: Index into that camera's keypoint list (links back to SIFT/ORB features)\n",
    "- **`pixel`**: Original pixel coordinates $(u, v)$ where the feature was detected\n",
    "\n",
    "### Example: Single Feature Observation\n",
    "\n",
    "```cpp\n",
    "// Camera 1 detected keypoint #45 at pixel location (523.7, 341.2)\n",
    "FeatureObservation obs1(1, 45, cv::Point2f(523.7, 341.2));\n",
    "\n",
    "// Access the data:\n",
    "// obs1.camera_idx = 1\n",
    "// obs1.feature_idx = 45\n",
    "// obs1.pixel.x = 523.7\n",
    "// obs1.pixel.y = 341.2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Track Struct\n",
    "\n",
    "```275:308:src/src/ceres_examples/virtual_cam_incremental_SfM.cpp\n",
    "struct Track {\n",
    "  int point3D_idx; // Index in globalPoints3D (-1 if not triangulated yet)\n",
    "  std::vector<FeatureObservation>\n",
    "      observations; // All cameras that see this point\n",
    "\n",
    "  Track() : point3D_idx(-1) {}\n",
    "\n",
    "  // Add a new observation to this track\n",
    "  void addObservation(int camera_idx, int feature_idx,\n",
    "                      const cv::Point2f &pixel) {\n",
    "    observations.emplace_back(camera_idx, feature_idx, pixel);\n",
    "  }\n",
    "\n",
    "  // Check if this track is observed by a specific camera\n",
    "  bool isObservedBy(int camera_idx) const {\n",
    "    for (const auto &obs : observations) {\n",
    "      if (obs.camera_idx == camera_idx)\n",
    "        return true;\n",
    "    }\n",
    "    return false;\n",
    "  }\n",
    "\n",
    "  // Get feature index in a specific camera (returns -1 if not observed)\n",
    "  int getFeatureIdx(int camera_idx) const {\n",
    "    for (const auto &obs : observations) {\n",
    "      if (obs.camera_idx == camera_idx)\n",
    "        return obs.feature_idx;\n",
    "    }\n",
    "    return -1;\n",
    "  }\n",
    "\n",
    "  // Get number of cameras observing this track\n",
    "  int numObservations() const { return static_cast<int>(observations.size()); }\n",
    "};\n",
    "```\n",
    "\n",
    "### Components:\n",
    "\n",
    "- **`point3D_idx`**: Index of the triangulated 3D point (or -1 if not yet triangulated)\n",
    "- **`observations`**: Vector of all `FeatureObservation`s that correspond to this 3D point\n",
    "- **Helper methods**: For querying track properties\n",
    "\n",
    "---\n",
    "\n",
    "## Numerical Examples\n",
    "\n",
    "### Example 1: Building a Track from Feature Matches\n",
    "\n",
    "**Scenario**: A corner of a building is visible in 3 cameras\n",
    "\n",
    "**Step 1: Feature Detection**\n",
    "\n",
    "Each camera detects keypoints independently:\n",
    "\n",
    "```cpp\n",
    "// Camera 0 keypoints\n",
    "std::vector<cv::KeyPoint> keypoints_cam0 = {\n",
    "    /* ... */\n",
    "    cv::KeyPoint(450.3, 302.1, 1.0),  // keypoint #17\n",
    "    /* ... */\n",
    "};\n",
    "\n",
    "// Camera 1 keypoints  \n",
    "std::vector<cv::KeyPoint> keypoints_cam1 = {\n",
    "    /* ... */\n",
    "    cv::KeyPoint(283.7, 218.4, 1.0),  // keypoint #23\n",
    "    /* ... */\n",
    "};\n",
    "\n",
    "// Camera 2 keypoints\n",
    "std::vector<cv::KeyPoint> keypoints_cam2 = {\n",
    "    /* ... */\n",
    "    cv::KeyPoint(521.9, 379.5, 1.0),  // keypoint #31\n",
    "    /* ... */\n",
    "};\n",
    "```\n",
    "\n",
    "**Step 2: Feature Matching**\n",
    "\n",
    "After matching (using SIFT descriptors, epipolar geometry, etc.), we discover that:\n",
    "- Camera 0's keypoint #17 ↔ Camera 1's keypoint #23\n",
    "- Camera 1's keypoint #23 ↔ Camera 2's keypoint #31\n",
    "- Camera 0's keypoint #17 ↔ Camera 2's keypoint #31\n",
    "\n",
    "These all correspond to the **same physical point** (the building corner).\n",
    "\n",
    "**Step 3: Create a Track**\n",
    "\n",
    "```cpp\n",
    "Track track_building_corner;\n",
    "\n",
    "// Add observation from camera 0\n",
    "track_building_corner.addObservation(0, 17, cv::Point2f(450.3, 302.1));\n",
    "\n",
    "// Add observation from camera 1\n",
    "track_building_corner.addObservation(1, 23, cv::Point2f(283.7, 218.4));\n",
    "\n",
    "// Add observation from camera 2\n",
    "track_building_corner.addObservation(2, 31, cv::Point2f(521.9, 379.5));\n",
    "\n",
    "// Initially, no 3D point assigned\n",
    "// track_building_corner.point3D_idx = -1\n",
    "```\n",
    "\n",
    "**Step 4: Triangulate the 3D Point**\n",
    "\n",
    "After triangulation, suppose we get 3D coordinates:\n",
    "\n",
    "```cpp\n",
    "cv::Point3f point3D(2.45, -1.33, 8.72);  // In camera 0 frame\n",
    "globalPoints3D.push_back(point3D);       // Stored at index 5\n",
    "\n",
    "// Link the track to the 3D point\n",
    "track_building_corner.point3D_idx = 5;\n",
    "```\n",
    "\n",
    "**Final Track State**:\n",
    "\n",
    "```cpp\n",
    "Track {\n",
    "    point3D_idx: 5,\n",
    "    observations: [\n",
    "        FeatureObservation{camera_idx: 0, feature_idx: 17, pixel: (450.3, 302.1)},\n",
    "        FeatureObservation{camera_idx: 1, feature_idx: 23, pixel: (283.7, 218.4)},\n",
    "        FeatureObservation{camera_idx: 2, feature_idx: 31, pixel: (521.9, 379.5)}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Using Track Helper Methods\n",
    "\n",
    "```cpp\n",
    "// Check if camera 1 sees this point\n",
    "bool visible = track_building_corner.isObservedBy(1);\n",
    "// Returns: true\n",
    "\n",
    "// Get the feature index in camera 1\n",
    "int feat_idx = track_building_corner.getFeatureIdx(1);\n",
    "// Returns: 23\n",
    "\n",
    "// Check how many cameras see this point\n",
    "int num_views = track_building_corner.numObservations();\n",
    "// Returns: 3\n",
    "\n",
    "// Check a camera that doesn't see this point\n",
    "int feat_idx_cam3 = track_building_corner.getFeatureIdx(3);\n",
    "// Returns: -1 (not observed)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Complete Multi-Track System\n",
    "\n",
    "**Scenario**: Incremental SfM with 3 cameras observing 4 3D points\n",
    "\n",
    "```cpp\n",
    "std::vector<Track> tracks;\n",
    "\n",
    "// ========================================\n",
    "// Track 0: Door handle (seen in all 3 cameras)\n",
    "// ========================================\n",
    "Track track0;\n",
    "track0.addObservation(0, 5,  cv::Point2f(320.0, 240.0));  // Cam 0, keypoint #5\n",
    "track0.addObservation(1, 12, cv::Point2f(315.5, 238.1));  // Cam 1, keypoint #12\n",
    "track0.addObservation(2, 8,  cv::Point2f(325.3, 242.7));  // Cam 2, keypoint #8\n",
    "track0.point3D_idx = 0;  // Triangulated to globalPoints3D[0] = (1.2, 0.5, 5.0)\n",
    "tracks.push_back(track0);\n",
    "\n",
    "// ========================================\n",
    "// Track 1: Window corner (seen in cameras 0 and 1)\n",
    "// ========================================\n",
    "Track track1;\n",
    "track1.addObservation(0, 18, cv::Point2f(450.0, 150.0));  // Cam 0, keypoint #18\n",
    "track1.addObservation(1, 27, cv::Point2f(445.2, 148.3));  // Cam 1, keypoint #27\n",
    "track1.point3D_idx = 1;  // Triangulated to globalPoints3D[1] = (3.1, -2.0, 6.5)\n",
    "tracks.push_back(track1);\n",
    "\n",
    "// ========================================\n",
    "// Track 2: Lamp post (seen in cameras 1 and 2)\n",
    "// ========================================\n",
    "Track track2;\n",
    "track2.addObservation(1, 34, cv::Point2f(200.0, 300.0));  // Cam 1, keypoint #34\n",
    "track2.addObservation(2, 15, cv::Point2f(205.8, 305.1));  // Cam 2, keypoint #15\n",
    "track2.point3D_idx = 2;  // Triangulated to globalPoints3D[2] = (-1.5, 1.8, 7.2)\n",
    "tracks.push_back(track2);\n",
    "\n",
    "// ========================================\n",
    "// Track 3: Tree branch (seen only in camera 0 - cannot triangulate yet!)\n",
    "// ========================================\n",
    "Track track3;\n",
    "track3.addObservation(0, 42, cv::Point2f(100.0, 400.0));  // Cam 0, keypoint #42\n",
    "track3.point3D_idx = -1;  // Not triangulated yet (need ≥2 views)\n",
    "tracks.push_back(track3);\n",
    "```\n",
    "\n",
    "**Summary Table**:\n",
    "\n",
    "| Track ID | 3D Point Index | # Observations | Cameras | Keypoints | Status |\n",
    "|----------|----------------|----------------|---------|-----------|---------|\n",
    "| 0 | 0 | 3 | 0, 1, 2 | 5, 12, 8 | ✅ Strong |\n",
    "| 1 | 1 | 2 | 0, 1 | 18, 27 | ✅ Valid |\n",
    "| 2 | 2 | 2 | 1, 2 | 34, 15 | ✅ Valid |\n",
    "| 3 | -1 | 1 | 0 | 42 | ❌ Insufficient |\n",
    "\n",
    "---\n",
    "\n",
    "### Example 4: Converting Tracks to Observations for Bundle Adjustment\n",
    "\n",
    "This is exactly what happens in the code around line 1267:\n",
    "\n",
    "```cpp\n",
    "// Camera intrinsics\n",
    "double fx = 800.0, fy = 800.0;\n",
    "double cx = 320.0, cy = 240.0;\n",
    "\n",
    "std::vector<Observation> observations;\n",
    "\n",
    "// Process each track\n",
    "for (size_t track_id = 0; track_id < tracks.size(); track_id++) {\n",
    "    const Track& track = tracks[track_id];\n",
    "    \n",
    "    // Skip tracks without 3D points\n",
    "    if (track.point3D_idx < 0) continue;\n",
    "    \n",
    "    // For each camera that sees this track\n",
    "    for (const auto& feat_obs : track.observations) {\n",
    "        Observation obs;\n",
    "        obs.camera_idx = feat_obs.camera_idx;\n",
    "        obs.point_idx = track.point3D_idx;\n",
    "        \n",
    "        // Convert pixel → Snavely normalized\n",
    "        obs.x = -(feat_obs.pixel.x - cx) / fx;\n",
    "        obs.y = -(feat_obs.pixel.y - cy) / fy;\n",
    "        \n",
    "        observations.push_back(obs);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**For Track 0** (from Example 3):\n",
    "\n",
    "```cpp\n",
    "// Observation 1: Camera 0 sees point 0\n",
    "Observation{\n",
    "    camera_idx: 0,\n",
    "    point_idx: 0,\n",
    "    x: -(320.0 - 320.0) / 800.0 = 0.0,\n",
    "    y: -(240.0 - 240.0) / 800.0 = 0.0\n",
    "}\n",
    "\n",
    "// Observation 2: Camera 1 sees point 0\n",
    "Observation{\n",
    "    camera_idx: 1,\n",
    "    point_idx: 0,\n",
    "    x: -(315.5 - 320.0) / 800.0 = 0.005625,\n",
    "    y: -(238.1 - 240.0) / 800.0 = 0.002375\n",
    "}\n",
    "\n",
    "// Observation 3: Camera 2 sees point 0\n",
    "Observation{\n",
    "    camera_idx: 2,\n",
    "    point_idx: 0,\n",
    "    x: -(325.3 - 320.0) / 800.0 = -0.006625,\n",
    "    y: -(242.7 - 240.0) / 800.0 = -0.003375\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences: Track vs Observation\n",
    "\n",
    "| Aspect | Track | Observation |\n",
    "|--------|-------|-------------|\n",
    "| **Purpose** | Group features that correspond to same 3D point | Link camera-point-measurement for optimization |\n",
    "| **Coordinates** | Stores pixel coordinates | Stores Snavely normalized coordinates |\n",
    "| **Structure** | One track → many features (multi-view) | One observation → one measurement |\n",
    "| **Usage** | Feature matching, triangulation, tracking | Bundle adjustment cost functions |\n",
    "| **Stage** | Early (feature matching phase) | Late (optimization phase) |\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use Tracks?\n",
    "\n",
    "1. **Feature Correspondence**: Tracks explicitly encode which features across different images correspond to the same 3D point\n",
    "2. **Incremental SfM**: When adding new cameras, you can check which existing tracks are visible\n",
    "3. **Outlier Detection**: Tracks with inconsistent reprojection errors can be rejected\n",
    "4. **Robustness**: Track length (number of observations) indicates point reliability\n",
    "\n",
    "The Track-based approach is industry-standard in modern SfM systems because it cleanly separates:\n",
    "- **Feature matching** (building tracks)\n",
    "- **Triangulation** (assigning 3D points to tracks)  \n",
    "- **Optimization** (converting tracks to observations for bundle adjustment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf43284-b772-4eb8-becd-c33835f82f6c",
   "metadata": {},
   "source": [
    "## Complete Numerical Example: 3-Camera Incremental SfM\n",
    "\n",
    "Let's reconstruct a simple scene: **a rectangular building facade** with 6 distinctive features (corners and windows).\n",
    "\n",
    "---\n",
    "\n",
    "## Scene Setup\n",
    "\n",
    "**Ground Truth 3D Points** (in original world frame):\n",
    "```cpp\n",
    "// Building facade points\n",
    "Point3D gt_points[6] = {\n",
    "    {0.0,  0.0, 5.0},  // P0: Bottom-left corner\n",
    "    {3.0,  0.0, 5.0},  // P1: Bottom-right corner\n",
    "    {0.0,  2.0, 5.0},  // P2: Top-left corner\n",
    "    {3.0,  2.0, 5.0},  // P3: Top-right corner\n",
    "    {1.0,  1.0, 5.0},  // P4: Window center-left\n",
    "    {2.0,  1.0, 5.0}   // P5: Window center-right\n",
    "};\n",
    "```\n",
    "\n",
    "**Camera Intrinsics** (all cameras identical):\n",
    "```cpp\n",
    "fx = fy = 800.0    // Focal length\n",
    "cx = cy = 320.0    // Principal point (640×480 image)\n",
    "k1 = k2 = 0.0      // No distortion\n",
    "```\n",
    "\n",
    "**Ground Truth Camera Poses**:\n",
    "```cpp\n",
    "Camera 0: R = I,  t = [0, 0, 0]      // Looking straight at facade\n",
    "Camera 1: R = I,  t = [1, 0, 0]      // Moved 1m to the right\n",
    "Camera 2: R = I,  t = [2, 0, 0]      // Moved 2m to the right\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 1: Feature Detection (each camera independently)\n",
    "\n",
    "**Camera 0 detects keypoints**:\n",
    "```cpp\n",
    "// Project 3D points onto Camera 0 image plane\n",
    "// Formula: u = fx * X/Z + cx,  v = fy * Y/Z + cy\n",
    "\n",
    "std::vector<cv::KeyPoint> keypoints_cam0 = {\n",
    "    cv::KeyPoint(320.0, 320.0, 1.0),  // kp_0: P0 → (0/5*800+320, 0/5*800+320)\n",
    "    cv::KeyPoint(800.0, 320.0, 1.0),  // kp_1: P1 → (3/5*800+320, ...)\n",
    "    cv::KeyPoint(320.0,  0.0,  1.0),  // kp_2: P2\n",
    "    cv::KeyPoint(800.0,  0.0,  1.0),  // kp_3: P3\n",
    "    cv::KeyPoint(480.0, 160.0, 1.0),  // kp_4: P4\n",
    "    cv::KeyPoint(640.0, 160.0, 1.0)   // kp_5: P5\n",
    "};\n",
    "```\n",
    "\n",
    "**Camera 1 detects keypoints** (shifted due to translation):\n",
    "```cpp\n",
    "// Camera 1 position: [1, 0, 0]\n",
    "// Effective 3D points in Cam1 frame: P_cam1 = P_world - [1,0,0]\n",
    "\n",
    "std::vector<cv::KeyPoint> keypoints_cam1 = {\n",
    "    cv::KeyPoint(160.0, 320.0, 1.0),  // kp_0: P0 = [-1,0,5] → u=-1/5*800+320=160\n",
    "    cv::KeyPoint(640.0, 320.0, 1.0),  // kp_1: P1 = [2,0,5] → u=2/5*800+320=640\n",
    "    cv::KeyPoint(160.0,  0.0,  1.0),  // kp_2: P2\n",
    "    cv::KeyPoint(640.0,  0.0,  1.0),  // kp_3: P3\n",
    "    cv::KeyPoint(320.0, 160.0, 1.0),  // kp_4: P4\n",
    "    cv::KeyPoint(480.0, 160.0, 1.0)   // kp_5: P5\n",
    "};\n",
    "```\n",
    "\n",
    "**Camera 2 detects keypoints**:\n",
    "```cpp\n",
    "// Camera 2 position: [2, 0, 0]\n",
    "\n",
    "std::vector<cv::KeyPoint> keypoints_cam2 = {\n",
    "    cv::KeyPoint(  0.0, 320.0, 1.0),  // kp_0: P0 = [-2,0,5] → u=-2/5*800+320=0\n",
    "    cv::KeyPoint(480.0, 320.0, 1.0),  // kp_1: P1 = [1,0,5] → u=1/5*800+320=480\n",
    "    cv::KeyPoint(  0.0,  0.0,  1.0),  // kp_2: P2\n",
    "    cv::KeyPoint(480.0,  0.0,  1.0),  // kp_3: P3\n",
    "    cv::KeyPoint(160.0, 160.0, 1.0),  // kp_4: P4\n",
    "    cv::KeyPoint(320.0, 160.0, 1.0)   // kp_5: P5\n",
    "};\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 2: Feature Matching Between Consecutive Camera Pairs\n",
    "\n",
    "**Matches between Camera 0 ↔ Camera 1**:\n",
    "```cpp\n",
    "std::vector<cv::DMatch> matches_01 = {\n",
    "    cv::DMatch(0, 0, 0.0),  // Cam0_kp0 ↔ Cam1_kp0 (both see P0)\n",
    "    cv::DMatch(1, 1, 0.0),  // Cam0_kp1 ↔ Cam1_kp1 (both see P1)\n",
    "    cv::DMatch(2, 2, 0.0),  // Cam0_kp2 ↔ Cam1_kp2 (both see P2)\n",
    "    cv::DMatch(3, 3, 0.0),  // Cam0_kp3 ↔ Cam1_kp3 (both see P3)\n",
    "    cv::DMatch(4, 4, 0.0),  // Cam0_kp4 ↔ Cam1_kp4 (both see P4)\n",
    "    cv::DMatch(5, 5, 0.0)   // Cam0_kp5 ↔ Cam1_kp5 (both see P5)\n",
    "};\n",
    "// All 6 points visible in both cameras\n",
    "```\n",
    "\n",
    "**Matches between Camera 1 ↔ Camera 2**:\n",
    "```cpp\n",
    "std::vector<cv::DMatch> matches_12 = {\n",
    "    cv::DMatch(0, 0, 0.0),  // Cam1_kp0 ↔ Cam2_kp0 (both see P0)\n",
    "    cv::DMatch(1, 1, 0.0),  // Cam1_kp1 ↔ Cam2_kp1 (both see P1)\n",
    "    cv::DMatch(2, 2, 0.0),  // Cam1_kp2 ↔ Cam2_kp2 (both see P2)\n",
    "    cv::DMatch(3, 3, 0.0),  // Cam1_kp3 ↔ Cam2_kp3 (both see P3)\n",
    "    cv::DMatch(4, 4, 0.0),  // Cam1_kp4 ↔ Cam2_kp4 (both see P4)\n",
    "    cv::DMatch(5, 5, 0.0)   // Cam1_kp5 ↔ Cam2_kp5 (both see P5)\n",
    "};\n",
    "// All 6 points still visible\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 3: Incremental Reconstruction - Initialize with Camera 0 & 1\n",
    "\n",
    "### 3.1: Compute Relative Pose (Camera 0 → Camera 1)\n",
    "\n",
    "```cpp\n",
    "// Essential matrix decomposition gives:\n",
    "cv::Mat R_01 = cv::Mat::eye(3, 3, CV_64F);  // No rotation\n",
    "cv::Mat t_01 = (cv::Mat_<double>(3,1) << 1.0, 0.0, 0.0);  // Translation\n",
    "```\n",
    "\n",
    "### 3.2: Set Global Poses (Camera 0 as world origin)\n",
    "\n",
    "```cpp\n",
    "// Camera 0: World reference\n",
    "cv::Mat R_cam0_global = cv::Mat::eye(3, 3, CV_64F);\n",
    "cv::Mat t_cam0_global = cv::Mat::zeros(3, 1, CV_64F);\n",
    "\n",
    "// Camera 1: Same as relative (since Cam0 = Identity)\n",
    "cv::Mat R_cam1_global = R_01.clone();\n",
    "cv::Mat t_cam1_global = t_01.clone();\n",
    "```\n",
    "\n",
    "### 3.3: Triangulate Initial 3D Points\n",
    "\n",
    "Using triangulation for P0 (bottom-left corner):\n",
    "\n",
    "```cpp\n",
    "// Camera 0: pixel (320, 320) → Snavely: (0.0, 0.0)\n",
    "// Camera 1: pixel (160, 320) → Snavely: (0.2, 0.0)\n",
    "\n",
    "// Triangulated result:\n",
    "cv::Point3f P0_triangulated(0.0, 0.0, 5.0);\n",
    "```\n",
    "\n",
    "All 6 points triangulated:\n",
    "\n",
    "```cpp\n",
    "std::vector<cv::Point3f> globalPoints3D = {\n",
    "    {0.0,  0.0, 5.0},  // Index 0: P0\n",
    "    {3.0,  0.0, 5.0},  // Index 1: P1\n",
    "    {0.0,  2.0, 5.0},  // Index 2: P2\n",
    "    {3.0,  2.0, 5.0},  // Index 3: P3\n",
    "    {1.0,  1.0, 5.0},  // Index 4: P4\n",
    "    {2.0,  1.0, 5.0}   // Index 5: P5\n",
    "};\n",
    "```\n",
    "\n",
    "### 3.4: Build Initial Tracks\n",
    "\n",
    "```cpp\n",
    "std::vector<Track> tracks;\n",
    "\n",
    "// ========================================\n",
    "// Track 0: Point P0 (Bottom-left corner)\n",
    "// ========================================\n",
    "Track track0;\n",
    "track0.addObservation(0, 0, cv::Point2f(320.0, 320.0));  // Cam0, kp_0\n",
    "track0.addObservation(1, 0, cv::Point2f(160.0, 320.0));  // Cam1, kp_0\n",
    "track0.point3D_idx = 0;  // Links to globalPoints3D[0]\n",
    "tracks.push_back(track0);\n",
    "\n",
    "// ========================================\n",
    "// Track 1: Point P1 (Bottom-right corner)\n",
    "// ========================================\n",
    "Track track1;\n",
    "track1.addObservation(0, 1, cv::Point2f(800.0, 320.0));\n",
    "track1.addObservation(1, 1, cv::Point2f(640.0, 320.0));\n",
    "track1.point3D_idx = 1;\n",
    "tracks.push_back(track1);\n",
    "\n",
    "// ========================================\n",
    "// Track 2: Point P2 (Top-left corner)\n",
    "// ========================================\n",
    "Track track2;\n",
    "track2.addObservation(0, 2, cv::Point2f(320.0, 0.0));\n",
    "track2.addObservation(1, 2, cv::Point2f(160.0, 0.0));\n",
    "track2.point3D_idx = 2;\n",
    "tracks.push_back(track2);\n",
    "\n",
    "// ========================================\n",
    "// Track 3: Point P3 (Top-right corner)\n",
    "// ========================================\n",
    "Track track3;\n",
    "track3.addObservation(0, 3, cv::Point2f(800.0, 0.0));\n",
    "track3.addObservation(1, 3, cv::Point2f(640.0, 0.0));\n",
    "track3.point3D_idx = 3;\n",
    "tracks.push_back(track3);\n",
    "\n",
    "// ========================================\n",
    "// Track 4: Point P4 (Window left)\n",
    "// ========================================\n",
    "Track track4;\n",
    "track4.addObservation(0, 4, cv::Point2f(480.0, 160.0));\n",
    "track4.addObservation(1, 4, cv::Point2f(320.0, 160.0));\n",
    "track4.point3D_idx = 4;\n",
    "tracks.push_back(track4);\n",
    "\n",
    "// ========================================\n",
    "// Track 5: Point P5 (Window right)\n",
    "// ========================================\n",
    "Track track5;\n",
    "track5.addObservation(0, 5, cv::Point2f(640.0, 160.0));\n",
    "track5.addObservation(1, 5, cv::Point2f(480.0, 160.0));\n",
    "track5.point3D_idx = 5;\n",
    "tracks.push_back(track5);\n",
    "```\n",
    "\n",
    "**Track Management Map** (for efficient lookup):\n",
    "```cpp\n",
    "// Map from (camera_idx, feature_idx) → track_id\n",
    "std::map<std::pair<int,int>, int> featureToTrack;\n",
    "\n",
    "// Camera 0 features\n",
    "featureToTrack[{0, 0}] = 0;  // Cam0_kp0 → Track 0\n",
    "featureToTrack[{0, 1}] = 1;  // Cam0_kp1 → Track 1\n",
    "featureToTrack[{0, 2}] = 2;\n",
    "featureToTrack[{0, 3}] = 3;\n",
    "featureToTrack[{0, 4}] = 4;\n",
    "featureToTrack[{0, 5}] = 5;\n",
    "\n",
    "// Camera 1 features\n",
    "featureToTrack[{1, 0}] = 0;  // Cam1_kp0 → Track 0\n",
    "featureToTrack[{1, 1}] = 1;\n",
    "featureToTrack[{1, 2}] = 2;\n",
    "featureToTrack[{1, 3}] = 3;\n",
    "featureToTrack[{1, 4}] = 4;\n",
    "featureToTrack[{1, 5}] = 5;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 4: Add Camera 2 Incrementally\n",
    "\n",
    "### 4.1: Compute Pose for Camera 2\n",
    "\n",
    "```cpp\n",
    "// Relative pose Camera 1 → Camera 2\n",
    "cv::Mat R_12 = cv::Mat::eye(3, 3, CV_64F);\n",
    "cv::Mat t_12 = (cv::Mat_<double>(3,1) << 1.0, 0.0, 0.0);\n",
    "\n",
    "// Global pose (chaining from Camera 0)\n",
    "cv::Mat R_cam2_global = R_cam1_global * R_12;\n",
    "cv::Mat t_cam2_global = R_cam1_global * t_12 + t_cam1_global;\n",
    "// Result: R = I, t = [2, 0, 0]\n",
    "```\n",
    "\n",
    "### 4.2: Extend Existing Tracks with Camera 2 Observations\n",
    "\n",
    "Since all 6 points are visible in Camera 2, we **extend** all existing tracks:\n",
    "\n",
    "```cpp\n",
    "// Process matches between Camera 1 and Camera 2\n",
    "for (const auto& match : matches_12) {\n",
    "    int feat_idx_cam1 = match.queryIdx;  // Feature in Camera 1\n",
    "    int feat_idx_cam2 = match.trainIdx;  // Feature in Camera 2\n",
    "    \n",
    "    // Find which track this belongs to, // Map from (camera_idx, feature_idx) → track_id\n",
    "    int track_id = featureToTrack[{1, feat_idx_cam1}];\n",
    "    \n",
    "    // Extend the track with Camera 2 observation\n",
    "    tracks[track_id].addObservation(2, feat_idx_cam2, \n",
    "                                    keypoints_cam2[feat_idx_cam2].pt);\n",
    "    \n",
    "    // Update map\n",
    "    featureToTrack[{2, feat_idx_cam2}] = track_id;\n",
    "}\n",
    "```\n",
    "\n",
    "**Updated Tracks** (now with 3 observations each):\n",
    "\n",
    "```cpp\n",
    "// Track 0: Now visible in 3 cameras\n",
    "Track {\n",
    "    point3D_idx: 0,\n",
    "    observations: [\n",
    "        {camera: 0, feature: 0, pixel: (320.0, 320.0)},\n",
    "        {camera: 1, feature: 0, pixel: (160.0, 320.0)},\n",
    "        {camera: 2, feature: 0, pixel: (0.0, 320.0)}    // NEW!\n",
    "    ]\n",
    "}\n",
    "\n",
    "// Track 1\n",
    "Track {\n",
    "    point3D_idx: 1,\n",
    "    observations: [\n",
    "        {camera: 0, feature: 1, pixel: (800.0, 320.0)},\n",
    "        {camera: 1, feature: 1, pixel: (640.0, 320.0)},\n",
    "        {camera: 2, feature: 1, pixel: (480.0, 320.0)}  // NEW!\n",
    "    ]\n",
    "}\n",
    "\n",
    "// ... (Tracks 2-5 similarly extended)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 5: Convert Tracks to Observations for Bundle Adjustment\n",
    "\n",
    "```cpp\n",
    "std::vector<Observation> observations;\n",
    "\n",
    "for (size_t track_id = 0; track_id < tracks.size(); track_id++) {\n",
    "    const Track& track = tracks[track_id];\n",
    "    \n",
    "    for (const auto& feat_obs : track.observations) {\n",
    "        Observation obs;\n",
    "        obs.camera_idx = feat_obs.camera_idx;\n",
    "        obs.point_idx = track.point3D_idx;\n",
    "        \n",
    "        // Convert pixel → Snavely normalized\n",
    "        obs.x = -(feat_obs.pixel.x - cx) / fx;\n",
    "        obs.y = -(feat_obs.pixel.y - cy) / fy;\n",
    "        \n",
    "        observations.push_back(obs);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Resulting Observations** (18 total: 6 points × 3 cameras):\n",
    "\n",
    "```cpp\n",
    "observations = {\n",
    "    // Track 0 observations\n",
    "    {camera: 0, point: 0, x: -(320.0-320)/800 = 0.0,     y: 0.0},\n",
    "    {camera: 1, point: 0, x: -(160.0-320)/800 = 0.2,     y: 0.0},\n",
    "    {camera: 2, point: 0, x: -(0.0-320)/800   = 0.4,     y: 0.0},\n",
    "    \n",
    "    // Track 1 observations\n",
    "    {camera: 0, point: 1, x: -(800.0-320)/800 = -0.6,    y: 0.0},\n",
    "    {camera: 1, point: 1, x: -(640.0-320)/800 = -0.4,    y: 0.0},\n",
    "    {camera: 2, point: 1, x: -(480.0-320)/800 = -0.2,    y: 0.0},\n",
    "    \n",
    "    // Track 2 observations\n",
    "    {camera: 0, point: 2, x: 0.0,   y: -(0.0-240)/800 = 0.3},\n",
    "    {camera: 1, point: 2, x: 0.2,   y: 0.3},\n",
    "    {camera: 2, point: 2, x: 0.4,   y: 0.3},\n",
    "    \n",
    "    // Track 3 observations\n",
    "    {camera: 0, point: 3, x: -0.6,  y: 0.3},\n",
    "    {camera: 1, point: 3, x: -0.4,  y: 0.3},\n",
    "    {camera: 2, point: 3, x: -0.2,  y: 0.3},\n",
    "    \n",
    "    // Track 4 observations\n",
    "    {camera: 0, point: 4, x: -(480.0-320)/800 = -0.2,  y: -(160.0-240)/800 = 0.1},\n",
    "    {camera: 1, point: 4, x: 0.0,   y: 0.1},\n",
    "    {camera: 2, point: 4, x: 0.2,   y: 0.1},\n",
    "    \n",
    "    // Track 5 observations\n",
    "    {camera: 0, point: 5, x: -0.4,  y: 0.1},\n",
    "    {camera: 1, point: 5, x: -0.2,  y: 0.1},\n",
    "    {camera: 2, point: 5, x: 0.0,   y: 0.1}\n",
    "};\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 6: Bundle Adjustment Setup\n",
    "\n",
    "```cpp\n",
    "// Prepare camera parameters (6 DOF per camera)\n",
    "// Camera 0 is FIXED (reference frame)\n",
    "std::vector<double> cameraParams;\n",
    "cameraParams.resize(3 * 6);  // 3 cameras × 6 parameters\n",
    "\n",
    "// Camera 0: [angle_axis(3), translation(3)] = [0,0,0, 0,0,0]\n",
    "cameraParams[0] = 0.0; cameraParams[1] = 0.0; cameraParams[2] = 0.0;\n",
    "cameraParams[3] = 0.0; cameraParams[4] = 0.0; cameraParams[5] = 0.0;\n",
    "\n",
    "// Camera 1: [0,0,0, 1,0,0]\n",
    "cameraParams[6] = 0.0; cameraParams[7] = 0.0; cameraParams[8] = 0.0;\n",
    "cameraParams[9] = 1.0; cameraParams[10] = 0.0; cameraParams[11] = 0.0;\n",
    "\n",
    "// Camera 2: [0,0,0, 2,0,0]\n",
    "cameraParams[12] = 0.0; cameraParams[13] = 0.0; cameraParams[14] = 0.0;\n",
    "cameraParams[15] = 2.0; cameraParams[16] = 0.0; cameraParams[17] = 0.0;\n",
    "\n",
    "// Prepare 3D point parameters\n",
    "std::vector<double> pointParams;\n",
    "pointParams.resize(3 * 6);  // 6 points × 3 coordinates\n",
    "\n",
    "pointParams[0] = 0.0; pointParams[1] = 0.0; pointParams[2] = 5.0;  // P0\n",
    "pointParams[3] = 3.0; pointParams[4] = 0.0; pointParams[5] = 5.0;  // P1\n",
    "pointParams[6] = 0.0; pointParams[7] = 2.0; pointParams[8] = 5.0;  // P2\n",
    "pointParams[9] = 3.0; pointParams[10] = 2.0; pointParams[11] = 5.0;  // P3\n",
    "pointParams[12] = 1.0; pointParams[13] = 1.0; pointParams[14] = 5.0;  // P4\n",
    "pointParams[15] = 2.0; pointParams[16] = 1.0; pointParams[17] = 5.0;  // P5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 7: Create Ceres Problem\n",
    "\n",
    "```cpp\n",
    "ceres::Problem problem;\n",
    "\n",
    "// Add residual blocks (one per observation)\n",
    "for (const Observation& obs : observations) {\n",
    "    \n",
    "    ceres::CostFunction* cost_function = \n",
    "        SnavelyReprojectionErrorFixedCamera::Create(\n",
    "            obs.x, obs.y, focal_length, k1, k2, obs_id);\n",
    "    \n",
    "    double* camera_ptr = &cameraParams[obs.camera_idx * 6];\n",
    "    double* point_ptr = &pointParams[obs.point_idx * 3];\n",
    "    \n",
    "    problem.AddResidualBlock(cost_function, nullptr, \n",
    "                            camera_ptr, point_ptr);\n",
    "}\n",
    "\n",
    "// Fix Camera 0 (world reference)\n",
    "problem.SetParameterBlockConstant(&cameraParams[0]);\n",
    "\n",
    "std::cout << \"Bundle Adjustment Problem:\" << std::endl;\n",
    "std::cout << \"  - Residual blocks: \" << observations.size() << \" (18)\" << std::endl;\n",
    "std::cout << \"  - Camera parameters: 3 cameras × 6 DOF = 18\" << std::endl;\n",
    "std::cout << \"    (Camera 0 fixed → optimizing 12 DOF)\" << std::endl;\n",
    "std::cout << \"  - Point parameters: 6 points × 3 coords = 18 DOF\" << std::endl;\n",
    "std::cout << \"  - Total DOF: 12 + 18 = 30\" << std::endl;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 8: Solve Bundle Adjustment\n",
    "\n",
    "```cpp\n",
    "ceres::Solver::Options options;\n",
    "options.linear_solver_type = ceres::DENSE_SCHUR;\n",
    "options.minimizer_progress_to_stdout = true;\n",
    "\n",
    "ceres::Solver::Summary summary;\n",
    "ceres::Solve(options, &problem, &summary);\n",
    "\n",
    "// Typical output:\n",
    "// Initial cost: 0.000000e+00  (perfect synthetic data)\n",
    "// Final cost:   0.000000e+00\n",
    "// Iterations: 1 (already at optimum)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## STEP 9: Print Reconstruction Summary\n",
    "\n",
    "```cpp\n",
    "printReconstructionSummary(\n",
    "    /* num_cameras */          3,\n",
    "    /* num_points */           6,\n",
    "    /* num_observations */     18,\n",
    "    /* ba_initial_cost */      0.0,\n",
    "    /* ba_final_cost */        0.0,\n",
    "    /* ba_iterations */        1,\n",
    "    /* avg_rotation_error */   0.0,      // degrees\n",
    "    /* avg_translation_error*/ 0.0,      // units\n",
    "    /* points_rms_error */     0.0,      // units\n",
    "    /* points_max_error */     0.0       // units\n",
    ");\n",
    "```\n",
    "\n",
    "**Console Output**:\n",
    "\n",
    "```\n",
    "================================================================================\n",
    "RECONSTRUCTION QUALITY SUMMARY\n",
    "================================================================================\n",
    "\n",
    "┌─────────────────────────────────────┬──────────────────────────┐\n",
    "│ Metric                              │ Value                    │\n",
    "├─────────────────────────────────────┼──────────────────────────┤\n",
    "│ Number of cameras                   │ 3                        │\n",
    "│ Number of 3D points                 │ 6                        │\n",
    "│ Total observations                  │ 18                       │\n",
    "├─────────────────────────────────────┼──────────────────────────┤\n",
    "│ BA initial cost                     │ 0.000000e+00             │\n",
    "│ BA final cost                       │ 0.000000e+00             │\n",
    "│ BA iterations                       │ 1                        │\n",
    "├─────────────────────────────────────┼──────────────────────────┤\n",
    "│ Avg camera rotation error           │ 0.0000 degrees           │\n",
    "│ Avg camera translation error        │ 0.000000 units           │\n",
    "├─────────────────────────────────────┼──────────────────────────┤\n",
    "│ 3D points RMS error                 │ 0.000000 units           │\n",
    "│ 3D points max error                 │ 0.000000 units           │\n",
    "└─────────────────────────────────────┴──────────────────────────┘\n",
    "\n",
    "✅ RECONSTRUCTION QUALITY: EXCELLENT (Near-perfect reconstruction!)\n",
    "\n",
    "================================================================================\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Complete Data Flow\n",
    "\n",
    "```\n",
    "FEATURE DETECTION\n",
    "    ↓\n",
    "3 Cameras × 6 Keypoints each = 18 total keypoints\n",
    "\n",
    "FEATURE MATCHING\n",
    "    ↓\n",
    "Cam0↔Cam1: 6 matches\n",
    "Cam1↔Cam2: 6 matches\n",
    "\n",
    "INCREMENTAL SfM\n",
    "    ↓\n",
    "Initialize: Cam0 + Cam1 → 6 tracks (2 obs each)\n",
    "Add Cam2:              → 6 tracks (3 obs each)\n",
    "\n",
    "TRACK STRUCTURE\n",
    "    ↓\n",
    "6 Tracks × 3 Observations = 18 FeatureObservations\n",
    "\n",
    "CONVERSION TO OBSERVATIONS\n",
    "    ↓\n",
    "18 Observations (Snavely normalized coordinates)\n",
    "\n",
    "BUNDLE ADJUSTMENT\n",
    "    ↓\n",
    "Optimize 30 DOF (12 camera + 18 point)\n",
    "Minimize reprojection error\n",
    "\n",
    "FINAL RECONSTRUCTION\n",
    "    ↓\n",
    "Recovered camera poses + 3D point cloud\n",
    "```\n",
    "\n",
    "This example demonstrates the complete pipeline from raw images to optimized 3D reconstruction using the track-based incremental SfM approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109f6fb-2f6d-4bc4-9ef0-8bed55abae6b",
   "metadata": {},
   "source": [
    "## Direct Answer\n",
    "\n",
    "### Relationship Between Projection Matrices and `points4D`\n",
    "\n",
    "In your code (lines 1076-1088):\n",
    "\n",
    "```cpp\n",
    "cv::Mat P_im1 = K * Rt_im1;  // Projection matrix for camera i-1\n",
    "cv::Mat P_i = K * Rt_i;      // Projection matrix for camera i\n",
    "\n",
    "cv::triangulatePoints(P_im1, P_i, pts_im1, pts_i, points4D);\n",
    "std::vector<cv::Point3f> newPoints_in_cam0 = convertHomogeneous(points4D);\n",
    "```\n",
    "\n",
    "### What Coordinate Frame Are the Points In?\n",
    "\n",
    "**Answer**: `points4D` contains points in **Camera 0 coordinate frame** (world frame).\n",
    "\n",
    "**Why?** Because both projection matrices are defined relative to Camera 0:\n",
    "- $P_{\\text{im1}} = K [R_{\\text{0→(i-1)}} | t_{\\text{0→(i-1)}}]$\n",
    "- $P_i = K [R_{\\text{0→i}} | t_{\\text{0→i}}]$\n",
    "\n",
    "Both transform from **Camera 0 frame → image plane**.\n",
    "\n",
    "---\n",
    "\n",
    "## The Mathematics\n",
    "\n",
    "### Projection Matrix Structure\n",
    "\n",
    "$$\n",
    "P = K [R | t] = \\begin{bmatrix}\n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_{11} & r_{12} & r_{13} & t_x \\\\\n",
    "r_{21} & r_{22} & r_{23} & t_y \\\\\n",
    "r_{31} & r_{32} & r_{33} & t_z\n",
    "\\end{bmatrix}_{3 \\times 4}\n",
    "$$\n",
    "\n",
    "**What it does**: Maps 3D points in **world coordinates** to 2D pixels:\n",
    "\n",
    "$$\n",
    "\\lambda \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = P \\begin{bmatrix} X_{\\text{world}} \\\\ Y_{\\text{world}} \\\\ Z_{\\text{world}} \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Triangulation (Inverse Process)\n",
    "\n",
    "**Given**:\n",
    "- Pixel $(u_1, v_1)$ in image 1 with projection matrix $P_1$\n",
    "- Pixel $(u_2, v_2)$ in image 2 with projection matrix $P_2$\n",
    "\n",
    "**Find**: 3D point $(X, Y, Z)$ that projects to both pixels.\n",
    "\n",
    "**Output**: `points4D` is a **4×N matrix** (homogeneous coordinates):\n",
    "\n",
    "$$\n",
    "\\text{points4D} = \\begin{bmatrix}\n",
    "X_1 & X_2 & \\cdots & X_N \\\\\n",
    "Y_1 & Y_2 & \\cdots & Y_N \\\\\n",
    "Z_1 & Z_2 & \\cdots & Z_N \\\\\n",
    "W_1 & W_2 & \\cdots & W_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Convert to 3D Euclidean\n",
    "\n",
    "```cpp\n",
    "for (int i = 0; i < points4D.cols; ++i) {\n",
    "    cv::Point3f point;\n",
    "    point.x = points4D.at<float>(0, i) / points4D.at<float>(3, i);  // X/W\n",
    "    point.y = points4D.at<float>(1, i) / points4D.at<float>(3, i);  // Y/W\n",
    "    point.z = points4D.at<float>(2, i) / points4D.at<float>(3, i);  // Z/W\n",
    "    // point is now in Camera 0 frame!\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Numerical Example\n",
    "\n",
    "**Scenario**: \n",
    "- Camera 0 at origin: $R = I$, $t = [0,0,0]^T$\n",
    "- Camera 1 at $[1, 0, 0]^T$: $R = I$, $t = [1,0,0]^T$\n",
    "- 3D point in world: $P = [2, 1, 8]^T$\n",
    "\n",
    "**Forward (projection)**:\n",
    "$$\n",
    "\\text{Camera 0: } (520, 340) \\text{ pixels}\n",
    "$$\n",
    "$$\n",
    "\\text{Camera 1: } (620, 340) \\text{ pixels}\n",
    "$$\n",
    "\n",
    "**Inverse (triangulation)**:\n",
    "```cpp\n",
    "cv::triangulatePoints(P0, P1, pts0, pts1, points4D);\n",
    "// points4D might be: [2, 1, 8, 1]^T (or scaled like [4, 2, 16, 2]^T)\n",
    "\n",
    "// After normalization:\n",
    "// X = 2.0, Y = 1.0, Z = 8.0  ✅ Correct!\n",
    "```\n",
    "\n",
    "**These coordinates $(2, 1, 8)$ are in Camera 0 frame.**\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points Summary\n",
    "\n",
    "1. **Projection Matrix $P = K[R|t]$**:\n",
    "   - Transforms from **world coordinates** to **pixel coordinates**\n",
    "   - $R, t$: World → Camera transformation\n",
    "\n",
    "2. **Output `points4D`**:\n",
    "   - **4×N matrix** in homogeneous coordinates\n",
    "   - Points are in the **reference frame** used by the projection matrices\n",
    "   - In your code: **Camera 0 frame** (world)\n",
    "\n",
    "3. **Homogeneous → Euclidean**:\n",
    "   - Divide by $W$: $(x, y, z) = (X/W, Y/W, Z/W)$\n",
    "   - **Critical step** - don't forget!\n",
    "\n",
    "4. **Your specific case**:\n",
    "   ```\n",
    "   P_im1 transforms: Camera 0 → Image(i-1)\n",
    "   P_i transforms:   Camera 0 → Image(i)\n",
    "   Result:          Points in Camera 0 frame\n",
    "   ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
